# Полное руководство по Kubernetes и облачным нативным технологиям

## Содержание

- [Полное руководство по Kubernetes и облачным нативным технологиям](#полное-руководство-по-kubernetes-и-облачным-нативным-технологиям)
  - [Содержание](#содержание)
- [ОСНОВЫ KUBERNETES](#основы-kubernetes)
  - [Введение в главу](#введение-в-главу)
  - [Архитектура Kubernetes](#архитектура-kubernetes)
    - [Основные компоненты](#основные-компоненты)
    - [Дополнительные концепции](#дополнительные-концепции)
  - [Настройка Kubernetes](#настройка-kubernetes)
  - [Kubernetes API](#kubernetes-api)
    - [1. Аутентификация](#1-аутентификация)
    - [2. Авторизация](#2-авторизация)
    - [3. Контроль доступа (Admission Control)](#3-контроль-доступа-admission-control)
  - [Запуск контейнеров в Kubernetes](#запуск-контейнеров-в-kubernetes)
    - [Пример с использованием containerd](#пример-с-использованием-containerd)
    - [Среды выполнения контейнеров](#среды-выполнения-контейнеров)
      - [containerd](#containerd)
      - [CRI-O](#cri-o)
      - [Docker](#docker)
  - [Сетевое взаимодействие](#сетевое-взаимодействие)
    - [Проблемы сетевого взаимодействия](#проблемы-сетевого-взаимодействия)
      - [Контейнер-к-контейнеру (Container-to-Container communications)](#контейнер-к-контейнеру-container-to-container-communications)
      - [Pod-к-Pod (Pod-to-Pod communications)](#pod-к-pod-pod-to-pod-communications)
      - [Pod-к-Сервису (Pod-to-Service communications)](#pod-к-сервису-pod-to-service-communications)
      - [Внешний-к-Сервису (External-to-Service communications)](#внешний-к-сервису-external-to-service-communications)
    - [Требования к сетевому взаимодействию](#требования-к-сетевому-взаимодействию)
    - [Реализация сетевого взаимодействия](#реализация-сетевого-взаимодействия)
    - [Network Policies](#network-policies)
  - [Планирование](#планирование)
    - [Основные моменты процесса планирования в Kubernetes](#основные-моменты-процесса-планирования-в-kubernetes)
    - [Ошибочные представления о Kubernetes](#ошибочные-представления-о-kubernetes)
- [ОРКЕСТРАЦИЯ КОНТЕЙНЕРОВ](#оркестрация-контейнеров)
  - [Введение в главу](#введение-в-главу-1)
  - [Использование контейнеров](#использование-контейнеров)
  - [Основы контейнеров](#основы-контейнеров)
    - [Современные технологии контейнеров](#современные-технологии-контейнеров)
    - [Namespaces](#namespaces)
    - [cgroups](#cgroups)
    - [Docker](#docker-1)
  - [Запуск контейнеров](#запуск-контейнеров)
    - [Запуск контейнеров с Docker](#запуск-контейнеров-с-docker)
    - [Стандарты runtime-spec и image-spec](#стандарты-runtime-spec-и-image-spec)
    - [Альтернативы Docker](#альтернативы-docker)
  - [Создание образов контейнеров](#создание-образов-контейнеров)
    - [Dockerfile](#dockerfile)
    - [Регистры контейнеров](#регистры-контейнеров)
  - [Безопасность](#безопасность)
    - [Проблемы с правами доступа](#проблемы-с-правами-доступа)
    - [Использование публичных образов](#использование-публичных-образов)
    - [Рекомендации по безопасности](#рекомендации-по-безопасности)
  - [Основы оркестрации контейнеров](#основы-оркестрации-контейнеров)
    - [Микросервисные архитектуры](#микросервисные-архитектуры)
    - [Управление и развертывание контейнеров](#управление-и-развертывание-контейнеров)
    - [Системы оркестрации контейнеров](#системы-оркестрации-контейнеров)
  - [Сетевое взаимодействие](#сетевое-взаимодействие-1)
    - [Сетевые пространства имён (Network Namespaces)](#сетевые-пространства-имён-network-namespaces)
    - [Доступ к приложению извне](#доступ-к-приложению-извне)
    - [Общение контейнеров между хостами](#общение-контейнеров-между-хостами)
    - [Интерфейс сетей контейнеров (CNI)](#интерфейс-сетей-контейнеров-cni)
  - [Обнаружение сервисов и DNS](#обнаружение-сервисов-и-dns)
    - [Проблемы в платформах оркестрации контейнеров](#проблемы-в-платформах-оркестрации-контейнеров)
    - [Решение: автоматизация](#решение-автоматизация)
    - [Подходы к обнаружению сервисов](#подходы-к-обнаружению-сервисов)
      - [DNS](#dns)
      - [Key-Value Store](#key-value-store)
  - [Сервисная сетка (Service Mesh)](#сервисная-сетка-service-mesh)
    - [Прокси-серверы](#прокси-серверы)
    - [Сервисная сетка (Service Mesh)](#сервисная-сетка-service-mesh-1)
    - [Пример шифрования](#пример-шифрования)
    - [Data Plane и Control Plane](#data-plane-и-control-plane)
    - [Интерфейс сервисной сетки (Service Mesh Interface, SMI)](#интерфейс-сервисной-сетки-service-mesh-interface-smi)
  - [Хранение данных](#хранение-данных)
    - [Образы контейнеров](#образы-контейнеров)
    - [Эфемерность контейнеров](#эфемерность-контейнеров)
    - [Объёмы (Volumes) в Kubernetes](#объёмы-volumes-в-kubernetes)
      - [Типы объёмов](#типы-объёмов)
      - [Проблемы изоляции](#проблемы-изоляции)
      - [Пример использования](#пример-использования)
    - [Оркестрация контейнеров и сохранение данных](#оркестрация-контейнеров-и-сохранение-данных)
    - [Интерфейс хранения контейнеров (CSI)](#интерфейс-хранения-контейнеров-csi)
- [ОБЛАЧНАЯ НАТИВНАЯ АРХИТЕКТУРА](#облачная-нативная-архитектура)
  - [Введение в главу](#введение-в-главу-2)
  - [Основы облачной нативной архитектуры](#основы-облачной-нативной-архитектуры)
    - [Традиционные и монолитные приложения](#традиционные-и-монолитные-приложения)
    - [Преимущества облачной нативной архитектуры](#преимущества-облачной-нативной-архитектуры)
    - [Архитектура монолита и микросервисов](#архитектура-монолита-и-микросервисов)
  - [Характеристики облачной нативной архитектуры](#характеристики-облачной-нативной-архитектуры)
    - [Характеристики облачной нативной архитектуры](#характеристики-облачной-нативной-архитектуры-1)
      - [Высокий уровень автоматизации](#высокий-уровень-автоматизации)
      - [Самовосстановление](#самовосстановление)
      - [Масштабируемость](#масштабируемость)
      - [Экономическая эффективность](#экономическая-эффективность)
      - [Легкость в обслуживании](#легкость-в-обслуживании)
      - [Безопасность по умолчанию](#безопасность-по-умолчанию)
    - [Методика «Двенадцати факторов» (Twelve-Factor App)](#методика-двенадцати-факторов-twelve-factor-app)
      - [Принципы методики](#принципы-методики)
  - [Автоскалирование](#автоскалирование)
    - [Горизонтальное и вертикальное масштабирование](#горизонтальное-и-вертикальное-масштабирование)
    - [Горизонтальное vs Вертикальное масштабирование](#горизонтальное-vs-вертикальное-масштабирование)
  - [Бессерверные технологии](#бессерверные-технологии)
    - [Введение](#введение)
    - [Коммерческие предложения](#коммерческие-предложения)
    - [Автоматическое масштабирование](#автоматическое-масштабирование)
    - [Комбинирование с другими платформами](#комбинирование-с-другими-платформами)
    - [Контейнерные образы и серверные системы](#контейнерные-образы-и-серверные-системы)
    - [Стандартизация и проект CloudEvents](#стандартизация-и-проект-cloudevents)
    - [Требования к приложениям](#требования-к-приложениям)
  - [Роли в облачных нативных средах и инженерия надежности сайтов (SRE)](#роли-в-облачных-нативных-средах-и-инженерия-надежности-сайтов-sre)
    - [Роли в облачных нативных средах](#роли-в-облачных-нативных-средах)
      - [Облачный архитектор](#облачный-архитектор)
      - [Инженер DevOps](#инженер-devops)
      - [Инженер по безопасности](#инженер-по-безопасности)
      - [Инженер DevSecOps](#инженер-devsecops)
      - [Инженер по данным](#инженер-по-данным)
      - [Full-Stack разработчик](#full-stack-разработчик)
      - [Инженер надежности сайта (SRE)](#инженер-надежности-сайта-sre)
    - [Метрики SRE](#метрики-sre)
  - [Сообщество и управление](#сообщество-и-управление)
    - [Комитет по техническому надзору (TOC)](#комитет-по-техническому-надзору-toc)
    - [Преимущества открытого управления](#преимущества-открытого-управления)
  - [Критерии выпуска CNCF v1.3](#критерии-выпуска-cncf-v13)
    - [Уровни зрелости проекта](#уровни-зрелости-проекта)
      - [Стадия песочницы](#стадия-песочницы)
      - [Стадия инкубации](#стадия-инкубации)
      - [Стадия выпуска](#стадия-выпуска)
- [РАБОТА С KUBERNETES](#работа-с-kubernetes)
  - [Введение в главу](#введение-в-главу-3)
  - [Объекты Kubernetes](#объекты-kubernetes)
  - [Взаимодействие с Kubernetes](#взаимодействие-с-kubernetes)
  - [Концепция Pod](#концепция-pod)
    - [Пример простого объекта Pod с двумя контейнерами](#пример-простого-объекта-pod-с-двумя-контейнерами)
    - [Пример Pod с init-контейнером](#пример-pod-с-init-контейнером)
  - [Жизненный цикл Pod](#жизненный-цикл-pod)
    - [Фазы жизненного цикла Pod](#фазы-жизненного-цикла-pod)
      - [Pending](#pending)
      - [Running](#running)
      - [Succeeded](#succeeded)
      - [Failed](#failed)
      - [Unknown](#unknown)
  - [Объекты нагрузки](#объекты-нагрузки)
    - [Объекты Kubernetes](#объекты-kubernetes-1)
    - [ReplicaSet](#replicaset)
    - [Deployment](#deployment)
    - [StatefulSet](#statefulset)
    - [DaemonSet](#daemonset)
    - [Job](#job)
    - [CronJob](#cronjob)
  - [Сетевые объекты](#сетевые-объекты)
    - [Типы сервисов](#типы-сервисов)
      - [ClusterIP](#clusterip)
      - [NodePort](#nodeport)
      - [LoadBalancer](#loadbalancer)
      - [ExternalName](#externalname)
      - [Headless Services](#headless-services)
    - [Ingress](#ingress)
    - [NetworkPolicy](#networkpolicy)
  - [Объекты томов и хранения](#объекты-томов-и-хранения)
    - [Пример монтирования тома hostPath](#пример-монтирования-тома-hostpath)
    - [Абстракции для работы с хранилищем](#абстракции-для-работы-с-хранилищем)
      - [PersistentVolume (PV)](#persistentvolume-pv)
      - [PersistentVolumeClaim (PVC)](#persistentvolumeclaim-pvc)
    - [Пример](#пример)
  - [Объекты конфигурации](#объекты-конфигурации)
    - [Пример ConfigMap с конфигурацией nginx](#пример-configmap-с-конфигурацией-nginx)
    - [Использование ConfigMap в Pod](#использование-configmap-в-pod)
  - [Объекты автоскалирования](#объекты-автоскалирования)
    - [Механизмы автоскалирования в Kubernetes](#механизмы-автоскалирования-в-kubernetes)
      - [Horizontal Pod Autoscaler (HPA)](#horizontal-pod-autoscaler-hpa)
      - [Cluster Autoscaler](#cluster-autoscaler)
      - [Vertical Pod Autoscaler (VPA)](#vertical-pod-autoscaler-vpa)
    - [Как Cluster Autoscaler увеличивает количество узлов](#как-cluster-autoscaler-увеличивает-количество-узлов)
    - [Требования для автоскалирования](#требования-для-автоскалирования)
    - [Автоскалирование на основе событий](#автоскалирование-на-основе-событий)
  - [Объекты планирования](#объекты-планирования)
    - [Методы выбора узлов для планирования Pod](#методы-выбора-узлов-для-планирования-pod)
      - [Поле nodeSelector, соответствующее меткам узла](#поле-nodeselector-соответствующее-меткам-узла)
      - [Affinity и Anti-Affinity в Kubernetes](#affinity-и-anti-affinity-в-kubernetes)
      - [Поле nodeName](#поле-nodename)
      - [Ограничения на распределение Pod по топологии](#ограничения-на-распределение-pod-по-топологии)
    - [Taints и Tolerations в Kubernetes](#taints-и-tolerations-в-kubernetes)
      - [Общий обзор](#общий-обзор)
      - [Taints](#taints)
      - [Tolerations](#tolerations)
      - [Зачем использовать Taints и Tolerations?](#зачем-использовать-taints-и-tolerations)
  - [Безопасность в Kubernetes](#безопасность-в-kubernetes)
    - [Доступ к API](#доступ-к-api)
      - [1. Аутентификация (токен)](#1-аутентификация-токен)
      - [2. Авторизация (RBAC)](#2-авторизация-rbac)
      - [3. Admission Controllers](#3-admission-controllers)
- [ДОСТАВКА ОБЛАЧНЫХ ПРИЛОЖЕНИЙ](#доставка-облачных-приложений)
  - [Введение в главу](#введение-в-главу-4)
  - [Основы доставки приложений](#основы-доставки-приложений)
  - [CI/CD](#cicd)
    - [Непрерывная Интеграция](#непрерывная-интеграция)
    - [Непрерывная Доставка](#непрерывная-доставка)
    - [CI/CD Pipeline](#cicd-pipeline)
    - [Популярные инструменты CI/CD](#популярные-инструменты-cicd)
  - [GitOps](#gitops)
    - [Push-based](#push-based)
    - [Pull-based](#pull-based)
- [НАБЛЮДАЕМОСТЬ В ОБЛАЧНЫХ СРЕДАХ](#наблюдаемость-в-облачных-средах)
  - [Введение в главу](#введение-в-главу-5)
  - [Наблюдаемость](#наблюдаемость)
  - [Телеметрия](#телеметрия)
    - [Логи](#логи)
    - [Метрики](#метрики)
    - [Трассировки](#трассировки)
  - [Логирование](#логирование)
    - [Логирование на уровне узла](#логирование-на-уровне-узла)
  - [Prometheus](#prometheus)
  - [Трассировка](#трассировка)
  - [Управление затратами](#управление-затратами)
    - [Автоматическая и ручная оптимизация](#автоматическая-и-ручная-оптимизация)
      - [Определение неиспользуемых и незадействованных ресурсов](#определение-неиспользуемых-и-незадействованных-ресурсов)
      - [Right-Sizing - Оптимизация размера](#right-sizing---оптимизация-размера)
      - [Reserved Instances - Резервирование ресурсов](#reserved-instances---резервирование-ресурсов)
      - [Spot Instances - Спотовые экземпляры](#spot-instances---спотовые-экземпляры)
- [KUBERNETES В ДВУХ СЛОВАХ](#kubernetes-в-двух-словах)
  - [Ключевые концепции и инструменты](#ключевые-концепции-и-инструменты)
  - [Популярные инструменты экосистемы](#популярные-инструменты-экосистемы)
  - [Рекомендации по масштабированию](#рекомендации-по-масштабированию)

# ОСНОВЫ KUBERNETES

## Введение в главу

Kubernetes — чрезвычайно популярная платформа с открытым исходным кодом для оркестрации контейнеров, которая автоматизирует развертывание, масштабирование и управление контейнеризованными рабочими нагрузками. В этой главе мы ознакомимся с базовой архитектурой кластера Kubernetes и его компонентами. Для углубленного изучения основ Kubernetes рекомендуем пройти бесплатный курс [Introduction to Kubernetes (LFS158x)](https://www.edx.org/course/introduction-to-kubernetes) от Linux Foundation на платформе edX.

Изначально разработанный Google, Kubernetes стал открытым в 2014 году, и вместе с выпуском v1.0 был передан в недавно созданный Cloud Native Computing Foundation (CNCF) в качестве первого проекта. Вокруг Kubernetes развивается множество технологий, связанных с облачными средами, включая контейнерные среды выполнения, мониторинг и инструменты доставки приложений.

Специалисты с навыками Kubernetes, как в области администрирования, так и разработки, высоко востребованы, поскольку Kubernetes стал центральным элементом многих современных облачных платформ.

## Архитектура Kubernetes

![Архитектура кластера Kubernetes](https://kubernetes.io/images/docs/kubernetes-cluster-architecture.svg)

Kubernetes функционирует как распределенная система и представляет собой кластер узлов, обеспечивая высокую масштабируемость и отказоустойчивость. Это позволяет системе эффективно управлять контейнеризированными приложениями в различных средах, от локальных до облачных развертываний.

### Основные компоненты

1. **Узлы контрольной плоскости (Control Plane Nodes)**
   
   Узлы контрольной плоскости управляют кластером и включают следующие компоненты:

![Архитектура kube-apiserver](https://raw.githubusercontent.com/nikolay-e/nikolay-e.github.io/master/tutorials/kubernetes/kube-apiserver-diagram.svg)

   - **kube-apiserver**
     - Центральный компонент, обрабатывающий API-запросы
     - Обеспечивает аутентификацию и авторизацию
     - Валидирует и обрабатывает запросы на изменение состояния кластера
     - **Пример**: При изменении политик безопасности администратор отправляет запрос через `kubectl`, который проходит через `kube-apiserver` для применения изменений

   - **etcd**
     - Распределенное key-value хранилище
     - Хранит все данные о состоянии кластера
     - Обеспечивает согласованность и надежность данных
     - **Пример**: При сбое узла, `etcd` позволяет восстановить состояние подов и сервисов, используя сохраненные данные

   - **kube-scheduler**
     - Планирует размещение подов на рабочих узлах
     - Учитывает требования к ресурсам, аффинити/анти-аффинити правила
     - Оптимизирует распределение нагрузки
     - **Пример**: Если приложение требует высокопроизводительных вычислений, `kube-scheduler` выберет узел с достаточным количеством CPU и GPU

   - **kube-controller-manager**
     - Управляет различными контроллерами кластера
     - Обеспечивает желаемое состояние ресурсов
     - Включает контроллеры для узлов, репликации, эндпоинтов и др.
     - **Пример**: Контроллер репликации автоматически создает новые поды, если количество запущенных подов не соответствует желаемому

   - **cloud-controller-manager** (опционально)
     - Взаимодействует с API облачных провайдеров
     - Управляет облачными ресурсами, такими как балансировщики нагрузки и хранилища
     - **Пример**: Автоматически создает балансировщик нагрузки в AWS при развертывании нового сервиса, который должен быть доступен извне

2. **Рабочие узлы (Worker Nodes)**
   
   Рабочие узлы выполняют приложения и включают:

   - **kubelet**
     - Агент, работающий на каждом узле
     - Обеспечивает работу подов
     - Взаимодействует с контейнерной средой выполнения
     - **Пример**: `kubelet` следит за тем, чтобы все поды на узле были запущены согласно их спецификациям в манифестах

   - **kube-proxy**
     - Управляет сетевыми правилами на узлах
     - Обеспечивает сетевое взаимодействие подов
     - Реализует абстракцию сервисов Kubernetes
     - **Пример**: `kube-proxy` настраивает правила IPTables, чтобы трафик, предназначенный для сервиса, был корректно перенаправлен на целевые поды

   - **Container Runtime**
     - Среда выполнения контейнеров (например, containerd, CRI-O)
     - Управляет жизненным циклом контейнеров
     - **Пример**: `Container Runtime` отвечает за запуск, остановку и управление контейнерами, запущенными на рабочих узлах

### Дополнительные концепции

- **Пространства имен (Namespaces)**
  - Логическое разделение ресурсов кластера
  - Позволяет изолировать ресурсы разных команд или проектов
  - **Пример**: Разработчики и тестировщики могут работать в отдельных пространствах имен, что минимизирует риск взаимного влияния их действий

- **Сетевая модель**
  - Обеспечивает связность между подами в кластере
  - Реализуется через CNI (Container Network Interface) плагины
  - **Пример**: Использование Calico для обеспечения сетевой политики, которая регулирует доступ между различными сервисами в кластере

- **Безопасность**
  - Многоуровневая модель безопасности
  - Включает аутентификацию, авторизацию, шифрование
  - Поддерживает RBAC (Role-Based Access Control)
  - **Пример**: Настройка RBAC для ограничения доступа операторов к конфиденциальным данным подов

## Настройка Kubernetes

Настройка кластера Kubernetes может быть выполнена разными методами. Создание тестового "кластера" может быть очень простым с использованием подходящих инструментов:

- **Minikube** — локальный однонодовый кластер для разработки и тестирования
- **kind (Kubernetes IN Docker)** — запускает локальные кластеры Kubernetes, используя Docker контейнеры в качестве узлов
- **MicroK8s** — легкий, сертифицированный CNCF дистрибутив Kubernetes от Canonical

Для настройки промышленного кластера на собственном оборудовании или виртуальных машинах, можно выбрать один из различных установщиков:

- **kubeadm** — официальный инструмент для быстрого создания кластера Kubernetes
- **kops** — инструмент для установки, обновления и управления производственными кластерами
- **kubespray** — инструмент на базе Ansible для развертывания и настройки Kubernetes

Некоторые вендоры предлагают готовые дистрибутивы Kubernetes с коммерческой поддержкой:

- **Rancher** — полноценная платформа для управления Kubernetes
- **k3s** — облегченный дистрибутив Kubernetes для edge и IoT устройств
- **OpenShift** — корпоративная платформа Kubernetes от Red Hat
- **VMWare Tanzu** — решение от VMWare для управления Kubernetes

Дистрибутивы часто выбирают конкретный подход и предлагают дополнительные инструменты, используя Kubernetes как центральный элемент своей платформы.

Если вы не хотите устанавливать и управлять кластером самостоятельно, можно использовать его у облачного провайдера:

- **Amazon Elastic Kubernetes Service (EKS)**
- **Google Kubernetes Engine (GKE)**
- **Microsoft Azure Kubernetes Service (AKS)**
- **DigitalOcean Kubernetes (DOKS)**

## Kubernetes API

Kubernetes API — самый важный компонент кластера Kubernetes. Без него невозможно взаимодействие с кластером, так как каждый пользователь и каждый компонент кластера взаимодействуют через api-server.

Прежде чем запрос будет обработан Kubernetes, он должен пройти три этапа:

### 1. Аутентификация

Запрашивающий должен предоставить средство идентификации для аутентификации через API. Обычно это делается с помощью цифрового подписанного сертификата (X.509) или с помощью внешней системы управления идентификацией. Пользователи Kubernetes всегда управляются внешне. Учетные записи сервисов (Service Accounts) могут использоваться для аутентификации технических пользователей.

### 2. Авторизация

На этом этапе решается, что запрашивающий имеет право делать. В Kubernetes это может быть выполнено с помощью Role Based Access Control (RBAC), который определяет, какие действия может выполнять пользователь над определенными ресурсами.

### 3. Контроль доступа (Admission Control)

На последнем этапе могут использоваться контроллеры доступа для изменения или проверки запроса. Например, если пользователь пытается использовать образ контейнера из ненадежного реестра, контроллер доступа может заблокировать этот запрос. Для управления контролем доступа извне можно использовать такие инструменты, как Open Policy Agent.

Kubernetes API реализован как RESTful интерфейс, доступный через HTTPS. Через API пользователь или сервис могут создавать, изменять, удалять или получать ресурсы, находящиеся в Kubernetes.

## Запуск контейнеров в Kubernetes

Как запуск контейнера на локальном компьютере отличается от запуска контейнеров в Kubernetes? В Kubernetes, вместо непосредственного запуска контейнеров, вы определяете Pod'ы как наименьшие вычислительные единицы, и Kubernetes переводит это в запущенные контейнеры. Мы подробнее рассмотрим Pod'ы позже, пока представьте их как оболочку вокруг контейнера.

Когда вы создаёте объект Pod в Kubernetes, в этот процесс вовлекаются несколько компонентов, пока контейнеры не будут запущены на узле.

### Пример с использованием containerd

![Running Containers in Kubernetes](https://kubernetes.io/images/docs/container-runtime.png)

В попытке позволить использовать другие среды выполнения контейнеров, помимо Docker, Kubernetes представил интерфейс Container Runtime Interface (CRI) в 2016 году.

### Среды выполнения контейнеров

#### containerd
containerd — лёгкая и производительная реализация для запуска контейнеров. Это, вероятно, самая популярная среда выполнения контейнеров на данный момент. Она используется всеми основными облачными провайдерами для продуктов Kubernetes как услуги.

#### CRI-O
CRI-O был создан Red Hat и имеет схожую кодовую базу, тесно связанную с podman и buildah. Он специально разработан для Kubernetes и является легковесной альтернативой другим средам выполнения.

#### Docker
Docker был стандартной средой выполнения в течение долгого времени, но никогда не был предназначен для оркестрации контейнеров. Использование Docker в качестве среды выполнения для Kubernetes было объявлено устаревшим и удалено в Kubernetes 1.24. Kubernetes имеет отличную документацию, отвечающую на все вопросы по этому поводу.

Идея containerd и CRI-O заключалась в предоставлении среды выполнения, которая содержит только абсолютно необходимые элементы для запуска контейнеров. Тем не менее, у них есть дополнительные функции, такие как возможность интеграции с инструментами для изоляции среды выполнения контейнеров. Эти инструменты пытаются решить проблему безопасности, связанную с совместным использованием ядра между несколькими контейнерами. Наиболее распространённые инструменты на данный момент:

- **gvisor**
  Созданный Google, предоставляет ядро приложения, которое находится между контейнеризованным процессом и ядром хоста.

- **Kata Containers**
  Безопасная среда выполнения, которая предоставляет легковесную виртуальную машину, но ведёт себя как контейнер.

## Сетевое взаимодействие

Сетевое взаимодействие в Kubernetes может быть сложным и трудным для понимания. Многие из этих концепций не связаны непосредственно с Kubernetes и были рассмотрены в главе «Оркестрация контейнеров». Основная проблема заключается в обеспечении коммуникации между множеством контейнеров через разные узлы. Kubernetes различает четыре различных сетевых проблемы, которые необходимо решить:

### Проблемы сетевого взаимодействия

#### Контейнер-к-контейнеру (Container-to-Container communications)
Эта проблема решается с помощью концепции Pod, о которой мы узнаем позже. Контейнеры внутри одного Pod могут взаимодействовать через локальный интерфейс (localhost).

#### Pod-к-Pod (Pod-to-Pod communications)
Эта проблема решается с помощью наложенной сети (overlay network), которая обеспечивает уникальный IP-адрес для каждого Pod и маршрутизацию между ними.

#### Pod-к-Сервису (Pod-to-Service communications)
Эта проблема решается с помощью kube-proxy и пакетного фильтра на узле, что позволяет подам обращаться к сервисам по стабильным IP-адресам и именам.

#### Внешний-к-Сервису (External-to-Service communications)
Эта проблема также решается с помощью kube-proxy и пакетного фильтра на узле, обеспечивая доступ к сервисам из внешних источников.

### Требования к сетевому взаимодействию

Существуют различные способы реализации сетевого взаимодействия в Kubernetes, но есть три важных требования:

1. Все Pod могут общаться друг с другом через узлы без использования NAT.
2. Все узлы могут общаться со всеми Pod без использования NAT.
3. IP-адрес, который видит Pod, такой же, как и IP-адрес, который видят другие Pod.

### Реализация сетевого взаимодействия

Для реализации сетевого взаимодействия вы можете выбрать из множества сетевых плагинов, таких как:

- **Project Calico** — предоставляет сетевое взаимодействие и сетевую политику безопасности для контейнеров
- **Weave** — создает виртуальную сеть, которая соединяет контейнеры через хосты
- **Cilium** — использует eBPF для обеспечения сетевого взаимодействия и безопасности на уровне L3-L7

В Kubernetes каждый Pod получает свой собственный IP-адрес, поэтому ручная конфигурация не требуется. Кроме того, большинство настроек Kubernetes включают сервер DNS под названием CoreDNS, который обеспечивает обнаружение сервисов и разрешение имён внутри кластера.

### Network Policies

По умолчанию каждый Pod может общаться с другими Pod в кластере Kubernetes. Однако если вы хотите контролировать поток трафика на уровне IP-адреса или порта, вам нужно использовать Network Policies. Network Policies действуют как внутренние брандмауэры кластера и позволяют определить, какой трафик разрешён к и от Pod, соответствующих определенному селектору.

Network Policies на основе IP-адресов определяются с использованием IP-блоков (CIDR-диапазонов). Они реализуются сетевым плагином, поэтому для их использования вы должны выбрать сетевое решение, которое поддерживает NetworkPolicy (например, Calico, Cilium). Создание ресурса NetworkPolicy без контроллера, который его реализует, не будет иметь никакого эффекта.

## Планирование

В своей простейшей форме планирование является подкатегорией оркестрации контейнеров и описывает процесс автоматического выбора правильного рабочего узла для запуска контейнеризованной рабочей нагрузки. В прошлом планирование было более ручной задачей, когда системный администратор выбирал подходящий сервер для приложения, отслеживая доступные серверы, их ёмкость и другие свойства, такие как местоположение.

В кластере Kubernetes компонентом, принимающим решение о планировании, является kube-scheduler, но он не несет ответственности за фактический запуск рабочей нагрузки. Процесс планирования в Kubernetes начинается, когда создается новый объект Pod. Помните, что Kubernetes использует декларативный подход, при котором Pod сначала описывается, затем планировщик выбирает узел, на котором Pod будет фактически запущен kubelet'ом и средой выполнения контейнеров.

### Основные моменты процесса планирования в Kubernetes

1. **Создание Pod**
   Процесс планирования начинается с создания нового объекта Pod.

2. **Декларативный подход**
   Kubernetes сначала описывает Pod, затем планировщик выбирает узел для запуска Pod.

3. **Выбор узла**
   Планировщик использует информацию о требованиях приложения (запросы на CPU и память) и свойствах узла для принятия решения. Например, пользователь может указать, что его приложение требует 2 ядра CPU, 4 ГБ памяти и желательно должно быть запущено на узле с быстрыми дисками.

4. **Фильтрация узлов**
   Планировщик фильтрует узлы, которые соответствуют этим требованиям. Если несколько узлов одинаково подходят, Kubernetes обычно планирует Pod на узле с наименьшим количеством уже запущенных Pod. Это поведение по умолчанию, если пользователь не указал дополнительных требований.

5. **Перепланирование**
   Если желаемое состояние не может быть достигнуто, например, из-за нехватки ресурсов на рабочих узлах, планировщик будет периодически повторять попытку найти подходящий узел до тех пор, пока состояние не будет установлено.

### Ошибочные представления о Kubernetes

Распространенное заблуждение о Kubernetes заключается в том, что он якобы обладает некоторой формой «искусственного интеллекта», анализирующего рабочую нагрузку и перемещающего Pod на основе потребления ресурсов, типа рабочей нагрузки и других факторов. На самом деле пользователь должен предоставить информацию о требованиях приложения, включая запросы на CPU и память, а также свойства узла.

Таким образом, процесс планирования в Kubernetes требует активного участия пользователя в предоставлении информации о требованиях к приложениям и ресурсам, что позволяет планировщику принимать оптимальные решения для размещения Pod в кластере.

# ОРКЕСТРАЦИЯ КОНТЕЙНЕРОВ

## Введение в главу

В этой главе мы рассмотрим проблемы и возможности оркестрации контейнеров, а также особые требования к сетевому взаимодействию и хранению данных.

Прежде чем мы начнем обсуждать оркестрацию контейнеров, давайте разберемся, что такое контейнер и почему он так полезен при создании и эксплуатации приложений.

## Использование контейнеров

История разработки приложений неразрывно связана с их упаковкой для различных платформ и операционных систем.

Рассмотрим простое веб-приложение, написанное на Python. Для запуска этого приложения система должна соответствовать определенным требованиям:

- Установка и настройка базовой операционной системы
- Установка основных пакетов Python
- Установка необходимых расширений Python
- Настройка сетевого взаимодействия
- Подключение к сторонним системам (база данных, кэш, хранилище)

Хотя разработчик лучше всего знает своё приложение и его зависимости, традиционно именно системный администратор предоставляет инфраструктуру, устанавливает зависимости и настраивает систему. Этот процесс подвержен ошибкам и сложен в поддержке, поэтому серверы обычно настраиваются для одной конкретной цели (например, только для базы данных или только для сервера приложений).

Для более эффективного использования серверного оборудования можно применять виртуальные машины, которые эмулируют полноценный сервер с CPU, памятью, хранилищем, сетью и операционной системой. Это позволяет запускать несколько изолированных серверов на одном физическом оборудовании.

До широкого распространения контейнеров виртуализация серверов была самым эффективным способом запуска изолированных приложений, но поскольку каждая виртуальная машина требует запуска полной операционной системы с ядром, это всегда приводит к определенным накладным расходам при запуске большого количества серверов.

Контейнеры решают обе эти проблемы: они упрощают управление зависимостями приложения и обеспечивают более эффективное выполнение по сравнению с виртуальными машинами.

## Основы контейнеров

Вопреки распространенному мнению, технологии контейнеров существуют давно. Одним из ранних предшественников современных технологий контейнеров является команда chroot, представленная в версии 7 Unix в 1979 году. Команда chroot изолирует процесс от корневой файловой системы, по сути "скрывая" файлы от процесса и симулируя новый корневой каталог. Изолированная среда называется "chroot-тюрьма", где файлы недоступны процессу, но все еще присутствуют в системе.

![Chroot directories](https://upload.wikimedia.org/wikipedia/commons/9/99/Chroot_directory_layout.png)

### Современные технологии контейнеров

Хотя chroot — довольно старая технология, она до сих пор используется во многих проектах. Современные технологии контейнеров воплощают тот же концепт, но в модернизированной версии и с множеством дополнительных функций.

Для более глубокой изоляции процессов, чем позволяет chroot, современные ядра Linux предоставляют такие функции, как namespaces и cgroups.

### Namespaces

Namespaces (пространства имен) используются для изоляции различных ресурсов, например, сети. Сетевой namespace может предоставить полную абстракцию сетевых интерфейсов и таблиц маршрутизации, позволяя процессу иметь собственный IP-адрес. Ядро Linux 5.6 предоставляет 8 типов namespaces:

- **pid** — изолирует идентификаторы процессов
- **net** — позволяет процессам иметь собственный сетевой стек, включая IP-адрес
- **mnt** — абстрагирует представление файловой системы и управляет точками монтирования
- **ipc** — изолирует именованные сегменты разделяемой памяти
- **user** — изолирует идентификаторы пользователей и групп
- **uts** — позволяет процессам иметь собственное имя хоста и доменное имя
- **cgroup** — позволяет процессу иметь собственный набор корневых директорий cgroup
- **time** — позволяет виртуализировать системные часы

### cgroups

cgroups (control groups) используются для организации процессов в иерархические группы и назначения им ресурсов, таких как память и CPU. Например, если вы хотите ограничить контейнер приложения 4 ГБ памяти, cgroups обеспечивают эти ограничения.

### Docker

Запущенный в 2013 году, Docker стал синонимом создания и запуска контейнеров. Хотя Docker не изобрел технологии, используемые для запуска контейнеров, он объединил существующие технологии таким образом, чтобы сделать контейнеры более удобными и доступными.

На первый взгляд контейнеры кажутся похожими на виртуальные машины, но они существенно отличаются. Виртуальные машины эмулируют полную машину, включая операционную систему и ядро, тогда как контейнеры используют ядро хост-машины и являются лишь изолированными процессами.

Виртуальные машины имеют накладные расходы на время загрузки, размер и использование ресурсов для работы операционной системы. Контейнеры же буквально являются процессами, как браузер на вашем компьютере, поэтому они запускаются гораздо быстрее и имеют меньший объем.

![Traditional Deployment vs Virtualized Deployment vs Container Deployment](https://d33wubrfki0l68.cloudfront.net/800c0e691b28c1c7e0139bbd5f78fc88d9e3a72a/fbec1/images/docs/container-ecosystem.png)

Во многих случаях вопрос не в том, использовать контейнеры или виртуальные машины, а в том, чтобы использовать обе технологии вместе, сочетая эффективность контейнеров с преимуществами безопасности, которые предоставляет более сильная изоляция виртуальных машин.

## Запуск контейнеров

Для запуска контейнеров, соответствующих отраслевым стандартам, необязательно использовать Docker; достаточно следовать стандарту OCI runtime-spec. Open Container Initiative также поддерживает эталонную реализацию среды выполнения контейнеров под названием runC. Эта низкоуровневая среда выполнения используется в различных инструментах для запуска контейнеров, включая сам Docker.

Если вы знакомы с объектно-ориентированным программированием, можно представить отношение между образом контейнера и запущенным контейнером как отношение между классом и экземпляром этого класса.

### Запуск контейнеров с Docker

С установленным Docker вы можете запускать контейнеры следующим образом:

```sh
docker run nginx
```

### Стандарты runtime-spec и image-spec

Стандарт runtime-spec тесно связан со стандартом image-spec, который мы рассмотрим позже. Он описывает, как распаковать образ контейнера и затем управлять полным жизненным циклом контейнера: от создания среды контейнера, запуска процесса до остановки и удаления.

### Альтернативы Docker

Существует множество альтернатив Docker для локальной машины. Некоторые из них предназначены только для создания образов, такие как buildah или kaniko, в то время как другие являются полными альтернативами Docker, например podman.

Podman предоставляет API, аналогичное Docker, и может использоваться в качестве замены. Более того, он обладает дополнительными функциями, такими как запуск контейнеров без привилегий root и использование концепции Pod, которую мы рассмотрим позже.

## Создание образов контейнеров

Почему контейнер называется контейнером? Эта метафора основана на транспортных контейнерах, стандартизированных по ISO 668. Стандартный формат транспортного контейнера позволяет легко укладывать их на корабль, разгружать краном и грузить на грузовик, независимо от содержимого. Многие термины в мире контейнеров и облачных технологий следуют этой морской теме.

Docker использовал компоненты для изоляции процессов, такие как namespaces и cgroups, но важной частью пазла, которая помогла контейнерам добиться прорыва, стало введение образов контейнеров.

Образы контейнеров делают контейнеры портативными и легкими для повторного использования на различных системах. Docker описывает образ контейнера следующим образом:

> "Docker контейнерный образ — это легкий, автономный, исполняемый пакет программного обеспечения, который включает все необходимое для запуска приложения: код, среду выполнения, системные инструменты, системные библиотеки и настройки."

В 2015 году формат образов, популяризированный Docker, был передан недавно основанной Open Container Initiative и также известен как OCI image-spec, который доступен на GitHub. Образы состоят из файловой системы и метаданных.

![Container Images](https://kubernetes.io/images/docs/container-ecosystem.png)

### Dockerfile

Образы могут быть созданы путем чтения инструкций из файла сборки, называемого Dockerfile. Инструкции похожи на процесс установки приложения на сервер. Вот пример Dockerfile, который контейнеризирует Python-скрипт:

```dockerfile
# Каждый образ контейнера начинается с базового образа.
# Это может быть ваша любимая дистрибуция Linux
FROM ubuntu:20.04 

# Выполните команды для добавления программного обеспечения и библиотек в ваш образ
# Здесь мы устанавливаем python3 и менеджер пакетов pip
RUN apt-get update && \
    apt-get -y install python3 python3-pip 

# Команда copy может использоваться для копирования вашего кода в образ
# Здесь мы копируем скрипт "my-app.py" в файловую систему контейнера
COPY my-app.py /app/ 

# Определяет рабочий каталог, в котором будет запускаться приложение
# С этого момента всё будет выполняться в /app
WORKDIR /app

# Процесс, который должен быть запущен при запуске контейнера
# В данном случае мы запускаем наше python приложение "my-app.py"
CMD ["python3","my-app.py"]
```

Если у вас установлен Docker, вы можете создать образ с помощью следующей команды:

```sh
docker build -t my-python-image -f Dockerfile .
```

С параметром `-t my-python-image` вы задаете тег имени для образа, а с `-f Dockerfile` указываете расположение Dockerfile. Это дает разработчикам возможность управлять всеми зависимостями своего приложения и упаковывать его готовым к запуску, вместо того чтобы делегировать эту задачу другому человеку или команде.

### Регистры контейнеров

Для распространения образов вы можете использовать регистр контейнеров — веб-сервер, где можно загружать и скачивать образы. Docker имеет встроенные команды push и pull:

```sh
docker push my-registry.com/my-python-image
docker pull my-registry.com/my-python-image
```

## Безопасность

Важно понимать, что контейнеры имеют другие требования к безопасности по сравнению с виртуальными машинами. Многие полагаются на изоляционные свойства контейнеров, но это может быть опасно.

Когда контейнеры запускаются на машине, они всегда используют одно и то же ядро, что представляет риск для всей системы, если контейнерам разрешено вызывать функции ядра, такие как завершение других процессов или модификация сети хоста путем создания правил маршрутизации. Подробнее о возможностях ядра можно узнать в [документации Docker](https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities).

### Проблемы с правами доступа

Одним из самых больших рисков безопасности, не только в области контейнеров, является выполнение процессов с избыточными привилегиями, особенно запуск процессов от имени root или администратора. К сожалению, эта проблема часто игнорировалась, и существует множество контейнеров, которые запускаются от имени root-пользователей.

### Использование публичных образов

Относительно новая поверхность атаки, введённая с контейнерами, — это использование публичных образов. Два из самых популярных публичных реестров образов — Docker Hub и Quay. Хотя они предоставляют общедоступные образы, необходимо убедиться, что эти образы не были изменены для включения вредоносного программного обеспечения.

### Рекомендации по безопасности

Sysdig опубликовал отличные рекомендации о том, как избежать многих проблем безопасности и создать безопасные образы контейнеров.

Безопасность в целом не может быть достигнута только на уровне контейнеров. Это непрерывный процесс, который необходимо постоянно адаптировать. Концепция "4C" облачной нативной безопасности (Cloud Native Security) дает общее представление о том, какие уровни нужно защищать при использовании контейнеров. Убедитесь, что вы охватываете каждый уровень, поскольку это эффективно защищает внутренний слой. [Документация Kubernetes](https://kubernetes.io/docs/concepts/security/overview/) является хорошей отправной точкой для понимания этих уровней.

![The 4C's of Cloud Native Security](https://d33wubrfki0l68.cloudfront.net/26763a07d171285fb9da805e0ae73eeb6e9fc9bb/3ce9f/docs/images/security/4cs-of-cloud-native-security.svg)

## Основы оркестрации контейнеров

Запустить несколько контейнеров на локальном компьютере или на одном сервере довольно просто, но способ использования контейнеров приводит к появлению новых задач, связанных с их эксплуатацией. Высокая эффективность концепции привела к тому, что приложения и сервисы становятся всё меньше, и современные приложения могут состоять из множества контейнеров.

### Микросервисные архитектуры

Множество небольших контейнеров, которые слабо связаны, изолированы и независимы, — это основа микросервисных архитектур. Эти небольшие контейнеры представляют собой автономные маленькие части бизнес-логики, которые являются элементами большого приложения.

### Управление и развертывание контейнеров

При управлении и развертывании большого количества контейнеров возникает необходимость в системе, которая поможет в управлении этими контейнерами. Проблемы, которые нужно решить, включают:

- Предоставление вычислительных ресурсов (виртуальных машин) для запуска контейнеров
- Планирование контейнеров на серверах эффективным образом
- Выделение ресурсов (CPU и память) для контейнеров
- Управление доступностью контейнеров и их заменой в случае сбоя
- Масштабирование контейнеров при увеличении нагрузки
- Обеспечение сетевого взаимодействия для подключения контейнеров друг к другу
- Предоставление хранилища для сохранения данных

### Системы оркестрации контейнеров

Системы оркестрации контейнеров позволяют создать кластер из нескольких серверов и размещать контейнеры на этом кластере. Большинство систем оркестрации контейнеров состоят из двух частей: контрольной плоскости, которая отвечает за управление контейнерами, и рабочих узлов, которые фактически запускают контейнеры.

За последние годы было создано несколько систем оркестрации контейнеров, но большинство из них сегодня уже не имеют большого значения, и индустрия выбрала Kubernetes как стандартную систему для оркестрации контейнеров.

## Сетевое взаимодействие

Микросервисная архитектура сильно зависит от сетевого взаимодействия. В отличие от монолитных приложений, микросервис реализует интерфейс, который можно вызвать для выполнения запроса. Например, у вас может быть сервис, который возвращает список продуктов в приложении электронной коммерции.

### Сетевые пространства имён (Network Namespaces)

Сетевые пространства имён позволяют каждому контейнеру иметь свой уникальный IP-адрес, поэтому несколько приложений могут открывать один и тот же сетевой порт. Например, у вас может быть несколько контейнеризованных веб-серверов, которые все слушают порт 8080.

### Доступ к приложению извне

Чтобы сделать приложение доступным извне хост-системы, контейнеры могут сопоставить порт контейнера с портом хост-системы (port mapping).

### Общение контейнеров между хостами

Для обеспечения связи между контейнерами на разных хостах можно использовать наложенную сеть (overlay network), которая помещает их в виртуальную сеть, охватывающую хост-системы. Это значительно упрощает взаимодействие контейнеров друг с другом, при этом системным администраторам не нужно настраивать сложные сетевые и маршрутизирующие связи между хостами и контейнерами.

Большинство наложенных сетей также управляют IP-адресами, что было бы трудоёмко при ручном выполнении. Наложенная сеть управляет тем, какой контейнер получает какой IP-адрес и как должен проходить трафик для доступа к отдельным контейнерам.

### Интерфейс сетей контейнеров (CNI)

Большинство современных реализаций сетевого взаимодействия контейнеров основаны на Container Network Interface (CNI). CNI — это стандарт, который можно использовать для написания или настройки сетевых плагинов и значительно упрощает замену различных плагинов в различных платформах оркестрации контейнеров.

## Обнаружение сервисов и DNS

Долгое время управление серверами в традиционных дата-центрах было управляемым. Многие системные администраторы даже запоминали все IP-адреса важных систем, с которыми им приходилось работать. Большие списки серверов, их имена хостов, IP-адреса и цели — всё это поддерживалось вручную и было повседневной практикой.

### Проблемы в платформах оркестрации контейнеров

В платформах оркестрации контейнеров ситуация гораздо сложнее:

- Сотни или тысячи контейнеров с индивидуальными IP-адресами
- Контейнеры развёртываются на различных хостах, в разных дата-центрах или даже в географически разных локациях
- Контейнерам или сервисам нужен DNS для общения, использование IP-адресов практически невозможно
- Информация о контейнерах должна быть удалена из системы при их удалении

### Решение: автоматизация

Решение этой проблемы — автоматизация. Вместо ручного поддержания списка серверов (или контейнеров) вся информация помещается в реестр сервисов (Service Registry). Нахождение других сервисов в сети и запрос информации о них называется обнаружением сервисов (Service Discovery).

### Подходы к обнаружению сервисов

#### DNS

Современные DNS-серверы, имеющие API для сервисов, могут использоваться для регистрации новых сервисов по мере их создания. Этот подход довольно прямолинейный, так как у большинства организаций уже есть DNS-серверы с соответствующими возможностями.

#### Key-Value Store

Использование строго консистентного хранилища данных, особенно для хранения информации о сервисах. Многие системы способны работать с высокой доступностью и сильными механизмами отказоустойчивости. Популярные решения, особенно для кластеризации, включают etcd, Consul и Apache Zookeeper.

## Сервисная сетка (Service Mesh)

Поскольку сетевое взаимодействие является ключевой частью микросервисов и контейнеров, оно может становиться очень сложным и непрозрачным для разработчиков и администраторов. Кроме того, желательны такие функции, как мониторинг, контроль доступа или шифрование сетевого трафика, когда контейнеры общаются друг с другом.

### Прокси-серверы

Вместо того чтобы встраивать всю эту функциональность в ваше приложение, вы можете запустить второй контейнер, который реализует эту функциональность. Программное обеспечение, которое можно использовать для управления сетевым трафиком, называется прокси-сервером. Это серверное приложение, которое находится между клиентом и сервером и может изменять или фильтровать сетевой трафик перед его достижением сервера. Популярные примеры — nginx, haproxy и envoy.

### Сервисная сетка (Service Mesh)

Сервисная сетка — это инфраструктурный слой для управления взаимодействием между микросервисами, обеспечивающий такие функции, как мониторинг, безопасность и контроль трафика без изменения кода приложений. Этот слой становится критически важным в сложных облачных средах, где динамичное масштабирование и взаимодействие микросервисов требуют более гибкого и мощного подхода к управлению сетью. Service Mesh позволяет централизованно управлять шифрованием, контролем доступа и распределением нагрузки, что значительно упрощает развертывание и поддержку масштабируемых приложений.

Используя инструменты как Istio, который включает в себя Envoy как прокси для управления трафиком, можно реализовать механизмы наблюдения и безопасного взаимодействия между службами. Istio, как проект CNCF, обеспечивает полную интеграцию с Kubernetes, что делает его популярным выбором для реализации сервисных сеток в облачных и локальных средах. Также другие инструменты, включая Linkerd и Consul, предоставляют схожие возможности, делая выбор оптимальной платформы зависимым от специфических требований проекта и предпочтений в экосистеме.

Consul, проект CNCF, в первую очередь используется для обнаружения сервисов и конфигурации. Consul предоставляет полнофункциональное решение сервисной сетки, но наиболее известен своими надежными возможностями обнаружения сервисов и конфигурационными службами, что делает его популярным выбором для управления микросервисными архитектурами, особенно в динамических средах.

### Пример шифрования

Рассмотрим шифрование как пример. Если два или более приложения должны шифровать свой трафик при взаимодействии друг с другом, это потребует добавления библиотек, настройки и управления цифровыми сертификатами, которые подтверждают личность задействованных приложений. Это требует значительных усилий и может быть подвержено ошибкам, если не сделано с особой тщательностью.

Когда используется сервисная сетка, приложения не общаются друг с другом напрямую, а трафик направляется через прокси. Наиболее популярные сервисные сетки в настоящее время — Istio и Linkerd. Хотя у них есть различия в реализации, архитектура одна и та же.

### Data Plane и Control Plane

Прокси в сервисной сетке образуют data plane (плоскость данных). Здесь реализуются сетевые правила и формируется поток трафика. Эти правила управляются централизованно в control plane (плоскости управления) сервисной сетки. Здесь вы определяете, как трафик будет течь от сервиса A к сервису B и какая конфигурация должна быть применена к прокси.

Таким образом, вместо написания кода и установки библиотек вы просто создаете конфигурационный файл, указывающий, что сервис A и сервис B должны всегда общаться зашифрованно. Конфигурация затем загружается в control plane и распространяется на data plane для применения нового правила.

### Интерфейс сервисной сетки (Service Mesh Interface, SMI)

Долгое время термин "сервисная сетка" описывал только основную идею о том, как трафик в контейнерных платформах может обрабатываться с помощью прокси. Проект Service Mesh Interface (SMI) направлен на определение спецификации того, как сервисная сетка от различных поставщиков может быть реализована.

С акцентом на Kubernetes, цель SMI — стандартизировать пользовательский опыт для сервисных сеток, а также создать стандарт для поставщиков, которые хотят интегрироваться с Kubernetes. Текущую спецификацию можно найти на GitHub.

## Хранение данных

С точки зрения хранения данных, у контейнеров есть значительный недостаток: они эфемерны (временны). Чтобы понять, что это значит, нужно понимать, что происходит, когда контейнер запускается из образа контейнера.

### Образы контейнеров

Образы контейнеров обычно доступны только для чтения и состоят из различных слоёв, которые включают всё, что вы добавили во время фазы сборки. Это гарантирует, что каждый раз при запуске контейнера из образа вы получаете одно и то же поведение и функциональность. Как вы можете себе представить, многим приложениям нужно записывать файлы. Чтобы разрешить запись файлов, при запуске контейнера из образа на него накладывается слой записи-чтения.

![Container Layers](https://docs.docker.com/v17.09/engine/userguide/storagedriver/images/aufs_layers.jpg)

### Эфемерность контейнеров

Проблема в том, что этот слой записи-чтения теряется, когда контейнер останавливается или удаляется. Это аналогично тому, как память компьютера стирается при его выключении. Чтобы сохранить данные, необходимо записать их на постоянное хранилище.

### Объёмы (Volumes) в Kubernetes

В Kubernetes, объёмы (`Volumes`) играют ключевую роль в управлении данными и состоянием приложений, работающих в контейнерах. Объёмы предоставляют постоянное хранилище, которое существует вне жизненного цикла отдельных контейнеров, позволяя данным сохраняться даже после перезапуска или удаления контейнеров. В отличие от временного хранилища, которое исчезает при удалении контейнера, объёмы предоставляют стабильное и безопасное место для хранения данных.

#### Типы объёмов

Kubernetes поддерживает множество типов объёмов, каждый из которых имеет свои особенности и применяется в зависимости от требований к инфраструктуре и данных:

- **EmptyDir**: Временный объём, создаваемый при создании Pod и существующий до его удаления. Полезен для хранения данных, используемых подами в рамках одного задания.
- **HostPath**: Позволяет подам получать доступ к файлам на хост-машине, что может быть полезно для доступа к системным библиотекам или для интеграции с системами на уровне хоста.
- **PersistentVolume (PV)** — блок хранилища в кластере Kubernetes, предварительно сконфигурированный администратором или динамически созданный посредством классов хранилища.
- **PersistentVolumeClaim (PVC)** — запрос на хранилище, который пользователи кластера могут использовать для запроса физического хранилища (PV) в кластере.
- **ConfigMap и Secret**: Используются для хранения конфигурационных данных и чувствительной информации соответственно, позволяя разделять конфигурацию и данные между разными подами.
- **NFS и другие сетевые хранилища**: Позволяют подам подключаться к удалённым хранилищам данных, обеспечивая централизованное управление данными и упрощение масштабирования.

#### Проблемы изоляции

Использование объёмов, особенно типа `HostPath`, может создать риски безопасности, так как это предоставляет контейнерам доступ к файловой системе хоста. Чтобы минимизировать эти риски, важно:

- Ограничивать доступ к объёмам с помощью политик безопасности подов (`PodSecurityPolicies`)
- Использовать минимально необходимые привилегии для доступа к хосту
- Применять сетевые политики для контроля трафика данных к и от подов

#### Пример использования

Пример конфигурации объёма типа `PersistentVolumeClaim`:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

Этот PVC может быть подключен к поду для обеспечения постоянного хранилища данных, что особенно важно для баз данных и других stateful приложений.

Объёмы в Kubernetes предоставляют гибкость в управлении данными и обеспечивают необходимую интеграцию и безопасность при работе с контейнеризированными приложениями.

### Оркестрация контейнеров и сохранение данных

При оркестрации множества контейнеров сохранение данных на хосте, где был запущен контейнер, может быть недостаточным. Часто данные должны быть доступны нескольким контейнерам, которые запущены на разных хост-системах, или когда контейнер запускается на другом хосте, он всё равно должен иметь доступ к своему объёму.

Системы оркестрации контейнеров, такие как Kubernetes, помогают смягчить эти проблемы, но всегда требуют надёжной системы хранения данных, подключенной к хост-серверам.

### Интерфейс хранения контейнеров (CSI)

Чтобы не отставать от неуклонного роста различных реализаций хранения данных, был разработан стандарт Container Storage Interface (CSI). Этот стандарт предлагает унифицированный интерфейс, который позволяет подключать различные системы хранения, будь то облачное или локальное хранение, к платформам оркестрации контейнеров.

# ОБЛАЧНАЯ НАТИВНАЯ АРХИТЕКТУРА

## Введение в главу

С ростом облачных вычислений требования и возможности для разработки, развертывания и проектирования приложений значительно изменились.

Облачные провайдеры предлагают множество услуг по запросу, начиная с простых (виртуальных) серверов, сетевого взаимодействия, хранения данных, баз данных и многого другого. Развертывание и управление этими сервисами очень удобно, будь то интерактивно или с помощью интерфейсов прикладного программирования (API).

В этой главе вы узнаете о принципах современной архитектуры приложений, часто называемой облачной нативной архитектурой (Cloud Native Architecture). Мы рассмотрим, что делает эти приложения нативными для облачных систем и чем они отличаются от традиционных подходов.

## Основы облачной нативной архитектуры

В основе идеи облачной нативной архитектуры лежит оптимизация программного обеспечения для экономической эффективности, надежности и быстрого вывода на рынок с использованием комбинации культурных, технологических и архитектурных паттернов.

Термин "облачная нативная архитектура" имеет различные определения, некоторые из которых сосредоточены на технологиях, а другие — на культурных аспектах.

Cloud Native Computing Foundation определяет это понятие следующим образом:

> "Облачные нативные технологии позволяют организациям создавать и запускать масштабируемые приложения в современных динамических средах, таких как публичные, частные и гибридные облака. Контейнеры, сервисные сетки, микросервисы, неизменная инфраструктура и декларативные API иллюстрируют этот подход.

> Эти техники позволяют создавать слабо связанные системы, которые являются устойчивыми, управляемыми и наблюдаемыми. В сочетании с надежной автоматизацией они позволяют инженерам часто и предсказуемо вносить изменения с минимальными усилиями. [...]"

### Традиционные и монолитные приложения

Традиционные или устаревшие приложения обычно разрабатываются с монолитным подходом, что означает, что они автономны и включают все функции и компоненты, необходимые для выполнения задачи. Монолитное приложение обычно имеет единую кодовую базу и предоставляется в виде одного исполняемого файла, который может быть запущен на сервере.

Если рассматривать ПО для электронной коммерции для онлайн-магазина, монолитное приложение будет включать в себя все функции, начиная от графического интерфейса, отображения продуктов, корзины покупок, оформления заказа, обработки заказов и многого другого.

Хотя может быть легко разрабатывать и развертывать приложение в этом формате, управлять сложностью, масштабировать разработку между несколькими командами, быстро внедрять изменения и эффективно масштабировать приложение при высокой нагрузке может быть значительно сложнее.

### Преимущества облачной нативной архитектуры

Облачная нативная архитектура предлагает решения для растущей сложности приложений и увеличивающегося спроса со стороны пользователей. Основная идея заключается в разбиении приложения на меньшие части, что делает их более управляемыми. Вместо того чтобы предоставлять все функции в одном приложении, у вас есть несколько раздельных приложений, которые взаимодействуют друг с другом через сеть. Если вернуться к примеру с онлайн-магазином, у вас может быть отдельное приложение для пользовательского интерфейса, оформления заказа и других функций. Эти небольшие, независимые приложения с чётко определённым набором функций часто называют микросервисами.

Это позволяет иметь несколько команд, каждая из которых отвечает за разные функции вашего приложения, а также управлять и масштабировать их индивидуально. Например, если много людей пытаются купить продукты, вы можете масштабировать сервисы, которые испытывают большую нагрузку, такие как корзина покупок и оформление заказа.

### Архитектура монолита и микросервисов

![Monolithic vs Microservices Architecture](https://d33wubrfki0l68.cloudfront.net/15aa19bfc9da32c536d92e74bba8f09e2ff74c1a/6c4a7/images/docs/microservices-vs-monolithic.png)

Облачная нативная архитектура имеет множество преимуществ, но также может быть очень сложной для интеграции, и поэтому должна соответствовать определенным требованиям для эффективной работы.

## Характеристики облачной нативной архитектуры

### Характеристики облачной нативной архитектуры

#### Высокий уровень автоматизации
Чтобы управлять всеми компонентами облачного нативного приложения, рекомендуется автоматизация на каждом этапе, от разработки до развертывания. Это достигается с помощью современных инструментов автоматизации и конвейеров непрерывной интеграции/непрерывной доставки (CI/CD), поддерживаемых системой контроля версий, такой как Git. Автоматизированное построение, тестирование и развертывание приложений и инфраструктуры с минимальным участием человека позволяет быстро, часто и постепенно вносить изменения в производственную среду. Надежная автоматизированная система также облегчает восстановление системы после сбоя.

#### Самовосстановление
Приложения и инфраструктура иногда выходят из строя. Это ожидаемое явление, поэтому фреймворки облачных нативных приложений и компоненты инфраструктуры включают проверки работоспособности, которые помогают мониторить приложение изнутри и автоматически перезапускать его при необходимости. Кроме того, поскольку приложение разделено на части, есть вероятность, что только некоторые части приложения перестанут работать или станут медленнее, в то время как другие части продолжат функционировать нормально.

#### Масштабируемость
Масштабирование приложения описывает процесс обработки большей нагрузки при сохранении приятного пользовательского опыта. Один из способов масштабирования — запуск нескольких копий одного и того же приложения и распределение нагрузки между ними. Автоматизация этого поведения на основе метрик приложения, таких как использование CPU или памяти, также обеспечивает доступность и производительность сервисов.

#### Экономическая эффективность
Как и масштабирование приложения для высоких нагрузок, уменьшение масштаба при низком трафике в сочетании с моделями ценообразования на основе использования облачных провайдеров может сэкономить затраты. Для оптимизации использования инфраструктуры системы оркестрации, такие как Kubernetes, могут помочь с более эффективным и плотным размещением приложений.

#### Легкость в обслуживании
Использование микросервисов позволяет разбивать приложения на более мелкие части, делая их более портативными, легче тестируемыми и распределяемыми между несколькими командами. Это упрощает обновление отдельных компонентов без необходимости перестраивать и развертывать все приложение. Каждый микросервис может развиваться и масштабироваться независимо, что обеспечивает более быстрые циклы выпуска и уменьшает риск при внесении изменений.

#### Безопасность по умолчанию
Облачные среды часто используются несколькими клиентами или командами, что требует различных моделей безопасности. В прошлом многие системы были разделены на разные зоны безопасности, которые запрещали доступ из разных сетей или группам пользователей. Оказавшись внутри зоны, можно было получить доступ к любой системе внутри. Современные подходы, такие как zero trust computing, смягчают эти риски, требуя аутентификации от каждого пользователя и процесса, независимо от их сетевого расположения.

### Методика «Двенадцати факторов» (Twelve-Factor App)

Хорошей отправной точкой для вашего пути в облачную нативную архитектуру является методика двенадцатифакторного приложения. Эта методика представляет собой набор принципов для создания масштабируемых и устойчивых веб-приложений, особенно подходящих для развёртывания в облачных средах. Эти принципы обеспечивают создание приложений, которые легко переносятся между различными средами и могут эффективно работать на современных платформах как сервис (PaaS).

Хотя эти паттерны и технологии изначально нацелены на оптимизацию для облачных платформ, они также предлагают значительные преимущества для локальных систем. Применение этих принципов не только улучшает управляемость и масштабируемость приложений, но и облегчает их миграцию в облако, делая переход более плавным и предсказуемым.

#### Принципы методики

1. **Кодовая база (Codebase)**  
   Единая кодовая база для всего приложения поддерживается в системе контроля версий, что обеспечивает единство и согласованность разработки. Вся кодовая база хранится в одном репозитории или в монорепозитории, что позволяет легко управлять изменениями и следить за историей разработки.

2. **Зависимости (Dependencies)**  
   Явное объявление и изоляция зависимостей. Приложение должно иметь возможность установки всех зависимостей через декларативный манифест, не полагаясь на системные инструменты или библиотеки.

3. **Конфигурация (Config)**  
   Хранение конфигурации в окружении отдельно от кода. Использование переменных окружения для конфигураций, которые могут меняться между развертываниями, вместо их жесткого кодирования.

4. **Вспомогательные сервисы (Backing Services)**  
   Рассматривать вспомогательные сервисы как прикрепленные ресурсы. Вспомогательные сервисы, такие как базы данных, системы очередей, кэширование, должны быть доступны для локальной и продуктивной среды без физических изменений в коде.

5. **Сборка, релиз, выполнение (Build, release, run)**  
   Строгое разделение стадий сборки приложения, его выпуска и выполнения. Каждый релиз должен иметь уникальный идентификатор и быть неизменным после создания.

6. **Процессы (Processes)**  
   Выполнение приложения как одного или более безсостоянийных процессов. Любые данные, требующие сохранения, должны храниться во внешнем хранилище или в постоянных бэкенд-сервисах.

7. **Порт привязки (Port binding)**  
   Экспорт сервисов через привязку к портам. Приложения должны быть самодостаточными и не зависеть от внешних веб-серверов для обработки запросов.

8. **Параллелизм (Concurrency)**  
   Масштабирование приложения через процессы. Это позволяет лучше использовать оборудование и увеличивать параллелизм, распределяя нагрузку между несколькими экземплярами приложения.

9. **Утилизация ресурсов (Disposability)**  
   Быстрый запуск и корректное завершение работы приложения. Процессы должны иметь минимальное время запуска и корректно обрабатывать сигналы завершения, что способствует устойчивости системы и более быстрому развертыванию.

10. **Равенство разработки и продакшена (Dev/prod parity)**  
    Держать среды разработки, стейджинга и продакшена максимально идентичными, чтобы избежать ошибок, связанных с различиями в этих средах. Это способствует непрерывной интеграции и доставке.

11. **Журналы (Logs)**  
    Рассматривать журналы как потоки событий. Журналы должны быть направлены в стандартный выходной поток и не обрабатываться приложением, а централизованно собираться и анализироваться.

12. **Административные процессы (Admin processes)**  
    Выполнение административных задач в виде одноразовых процессов в среде, максимально приближенной к продуктивной. Такие задачи как миграция базы данных или исполнение разовых скриптов должны выполняться в той же среде, что и само приложение.

Эти принципы помогают разработчикам создавать приложения, которые легко масштабируются и обновляются, обеспечивая при этом высокую производительность и надежность.

## Автоскалирование

Паттерн автоскалирования описывает динамическую настройку ресурсов в зависимости от текущего спроса. CPU и память являются очевидными метриками для определения момента масштабирования приложений при увеличении или уменьшении нагрузки, но также можно использовать другие подходы, основанные на времени или бизнес-метриках, для увеличения или уменьшения количества экземпляров ваших сервисов.

### Горизонтальное и вертикальное масштабирование

Когда мы говорим об автоскалировании, обычно имеем в виду горизонтальное масштабирование — процесс запуска новых вычислительных ресурсов, такими как новые экземпляры приложения, виртуальные машины или даже новые стойки серверов и другое оборудование.

Вертикальное масштабирование, с другой стороны, описывает изменение размеров существующего оборудования, что работает только в пределах определённых аппаратных ограничений как для физических серверов, так и для виртуальных машин. Виртуальные машины и процессы могут легко масштабироваться вверх, получая больше CPU и памяти, но верхний предел определяется вычислительной мощностью и объёмом памяти базового оборудования. Само физическое оборудование может быть расширено, например, путём добавления большего количества оперативной памяти, но только до тех пор, пока все слоты памяти не будут заняты.

Чтобы проиллюстрировать разницу между вертикальным и горизонтальным масштабированием, представьте, что вам нужно нести тяжёлый предмет, который вы не можете поднять. Вы можете нарастить мышцы, чтобы нести его самостоятельно, но у вашего тела есть верхний предел силы. Это вертикальное масштабирование. Вы также можете позвать друзей, чтобы разделить работу между несколькими людьми. Это горизонтальное масштабирование.

### Горизонтальное vs Вертикальное масштабирование

Настройка автоскалирования в различных средах требует определения минимального и максимального предела экземпляров (виртуальных машин или контейнеров) и выбора метрик, которые запускают процесс масштабирования. Для правильной настройки масштабирования часто необходимо проводить множество нагрузочных тестов, максимально приближенных к производственным условиям, и анализировать поведение и балансировку нагрузки при масштабировании приложения.

Облачные среды с моделями ценообразования по запросу на основе использования предоставляют очень эффективные платформы для автоматического масштабирования с возможностью предоставления большого количества ресурсов в течение нескольких секунд или даже масштабирования до нуля, если ресурсы временно не требуются.

Даже если масштабирование приложений и базовой инфраструктуры изначально не автоматизировано, возможность масштабировать приложение может повысить доступность и устойчивость сервисов даже в более традиционных средах.

## Бессерверные технологии

### Введение

Несмотря на термин "бессерверный", серверы, безусловно, по-прежнему необходимы как основа для приложений. Облачные провайдеры утверждают, что развертывание приложений стало очень простым, но для этого требуется подготовка и настройка нескольких ресурсов, таких как сеть, виртуальные машины, операционные системы и балансировщики нагрузки даже для простого веб-приложения. Идея бессерверных вычислений заключается в освобождении разработчиков от этих сложных задач. Проще говоря, вы предоставляете только код приложения, а облачный провайдер выбирает подходящую среду для его выполнения.

### Коммерческие предложения

Все популярные облачные провайдеры предлагают одно или несколько коммерческих решений собственных бессерверных сред и подмножество бессерверных технологий, также известных как Function as a Service (FaaS). Облачный провайдер абстрагирует базовую инфраструктуру, чтобы разработчики могли развертывать программное обеспечение, загружая свой код в виде файлов .zip или предоставляя образ контейнера.

### Автоматическое масштабирование

В отличие от других моделей облачных вычислений, бессерверные вычисления имеют еще более сильный акцент на предоставление и масштабирование приложений по запросу. Автоматическое масштабирование является фундаментальным концептом бессерверных технологий и может включать масштабирование и предоставление ресурсов на основе событий, таких как входящие запросы. Это позволяет более точно рассчитывать стоимость на основе конкретных событий, вместо традиционного расчета по времени работы.

### Комбинирование с другими платформами

Вместо полной замены платформ оркестрации контейнеров или более традиционных виртуальных машин, системы FaaS часто используются в сочетании или как расширение существующих платформ, поскольку они обеспечивают очень быстрое развертывание и отлично подходят для тестирования и экспериментальных сред.

### Контейнерные образы и серверные системы

Хотя контейнерные образы являются стандартизированным способом упаковки программного обеспечения для FaaS или бессерверных систем, такие проекты как Knative, построенные на основе Kubernetes, позволяют расширять существующие платформы функциональностью бессерверных вычислений. Это может быть эффективным решением для использования в частных облаках и локальных средах. Стоит отметить, что это может упростить работу разработчика, но усложнить эксплуатацию облачной платформы.

### Стандартизация и проект CloudEvents

Хотя бессерверные технологии имеют множество преимуществ, изначально они сталкивались с проблемами стандартизации. Многие облачные провайдеры предлагают собственные решения, что затрудняет переход между различными платформами. Для решения этих проблем был основан проект CloudEvents, который предоставляет спецификацию структуры данных событий. События являются основой для масштабирования бессерверных рабочих нагрузок или запуска соответствующих функций. Чем больше поставщиков и инструментов примут такой стандарт, тем легче будет использовать бессерверные и событийно-ориентированные архитектуры на нескольких платформах.

CloudEvents — это спецификация для описания данных событий общим образом, которая стремится значительно упростить объявление и доставку событий между сервисами, платформами и за их пределами.

CloudEvents — это инициатива, которая все еще находится в активной разработке, но уже получила значительное внимание со стороны отрасли, от крупных облачных провайдеров до популярных SaaS-компаний. Спецификация теперь находится под управлением Cloud Native Computing Foundation.

### Требования к приложениям

Приложения, написанные для бессерверных платформ, имеют еще более строгие требования к облачной нативной архитектуре, но в то же время могут получить от них наибольшую выгоду. Написание небольших, бессостоящих приложений делает их идеальными для обработки событий или потоков данных, запланированных задач, реализации бизнес-логики или пакетной обработки.

## Роли в облачных нативных средах и инженерия надежности сайтов (SRE)

Технологические и культурные изменения, произошедшие за последние два десятилетия, привели к корректировке задач и описаний рабочих мест. Традиционные роли включали системного администратора, сетевого администратора, администратора баз данных, разработчика программного обеспечения или менеджера по тестированию.

Работы в области облачных вычислений сложнее четко определить, и границы между ними более размыты, так как ответственность часто распределяется между несколькими специалистами из разных областей с различными наборами навыков.

### Роли в облачных нативных средах

#### Облачный архитектор
Отвечает за внедрение облачных технологий, проектирование ландшафта приложений и инфраструктуры с упором на безопасность, масштабируемость и механизмы развертывания. Определяет общую стратегию использования облачных сервисов и технологий.

#### Инженер DevOps
Часто описывается как соединение ролей разработчика и администратора, но это упрощенное представление. Инженеры DevOps используют инструменты и процессы, которые уравновешивают разработку программного обеспечения и операционную деятельность. Они оптимизируют весь жизненный цикл создания, тестирования и развертывания программного обеспечения.

#### Инженер по безопасности
Эта роль осталась наиболее узнаваемой, но значительно изменилась по содержанию. Облачные технологии создали новые векторы атак, и в современных условиях роль безопасности должна быть интегрирована как неотъемлемая часть команды, а не изолированный отдел.

#### Инженер DevSecOps
В стремлении сделать безопасность неотъемлемой частью современных ИТ-сред, инженер DevSecOps объединяет роли DevOps и специалиста по безопасности. Эта роль часто выступает связующим звеном между традиционными командами разработчиков и специалистами по безопасности.

#### Инженер по данным
Инженеры по данным решают задачи сбора, хранения и анализа огромных объемов данных, генерируемых в современных системах. Это включает обеспечение и управление специализированной инфраструктурой, а также непосредственную работу с данными.

#### Full-Stack разработчик
Универсальный специалист, который комфортно работает как с фронтендом, так и с бэкендом приложений, а также имеет представление о базовой инфраструктуре и её требованиях.

#### Инженер надежности сайта (SRE)
Роль с более четким определением — инженер надежности сайта (Site Reliability Engineer). SRE была создана около 2003 года в Google и стала важной функцией для многих организаций. Основная цель SRE — создавать и поддерживать надежное и масштабируемое программное обеспечение. Для достижения этой цели используются подходы разработки программного обеспечения для решения операционных проблем и автоматизации операционных задач.

### Метрики SRE

Для измерения производительности и надежности SRE используют три основные метрики:

- **Цели уровня обслуживания (SLO)**: "Устанавливают целевой уровень надежности вашего сервиса." Например, цель по задержке сервиса менее 100 мс.
- **Индикаторы уровня обслуживания (SLI)**: "Тщательно определенная количественная мера некоторого аспекта уровня обслуживания." Например, фактически измеренное время отклика на запрос.
- **Соглашения об уровне обслуживания (SLA)**: "Являются явным или неявным контрактом с пользователями, который включает последствия выполнения или невыполнения SLO. Последствия наиболее легко распознаются, когда они финансовые – возмещение или штраф – но могут принимать и другие формы." Определяют, что происходит, если SLO не достигаются.

Вокруг этих метрик SRE могут определить бюджет ошибок — допустимое количество (или время) ошибок, которое может иметь приложение, прежде чем будут приняты меры, например, приостановка развертываний в производственную среду.

## Сообщество и управление

Множество проектов с открытым исходным кодом, ставших отраслевыми стандартами, размещаются и поддерживаются Cloud Native Computing Foundation (CNCF). Проекты классифицируются по степени зрелости и проходят через этапы песочницы и инкубации перед выпуском. Сильное сообщество CNCF поддерживает проекты на протяжении всего их жизненного цикла, начиная с таких аспектов, как публичная видимость и классификация в ландшафте CNCF, и до момента, когда проекты созревают для использования в производственных условиях.

### Комитет по техническому надзору (TOC)

CNCF имеет Комитет по техническому надзору (Technical Oversight Committee, TOC), который отвечает за определение и поддержание технического видения, утверждение новых проектов, учет отзывов комитета конечных пользователей и определение общих практик для реализации в проектах CNCF.

TOC не контролирует проекты напрямую, а поощряет их к самоорганизации и общему владению сообществом, придерживаясь принципа "минимального жизнеспособного управления". Руководящие принципы для этих проектов включают методы поддержки, рецензирования и выпуска, а также формирование групп пользователей и рабочих групп. Поскольку проекты с открытым исходным кодом зависят от своих сообществ, модель управления значительно отличается от традиционных подходов.

### Преимущества открытого управления

- **Публичная видимость**: Участие в CNCF предоставляет проектам значительную видимость и признание в индустрии.
- **Поддержка на всех этапах**: Проекты получают поддержку на каждом этапе развития, от начальной идеи до зрелого продукта.
- **Менторство и сотрудничество**: Возможность получать советы и сотрудничать с лидерами отрасли и другими проектами.
- **Управление сообществом**: Проекты управляются сообществом, что способствует более широкому участию и разнообразию мнений и подходов.

Проекты CNCF часто становятся основой для создания коммерческих продуктов и предложений облачных провайдеров, что также способствует их быстрому развитию и внедрению в промышленность.

## Критерии выпуска CNCF v1.3

Каждой инициативе CNCF присваивается уровень зрелости. Проекты, предлагаемые CNCF, должны указывать предпочитаемый уровень зрелости. Проект должен получить две трети голосов, чтобы быть утвержденным как инкубационный или выпускной. Все голоса за выпускной этап считаются голосами за инкубационный проект, если не набирается требуемое большинство для выпуска. Все голоса за инкубационный или выпускной этап считаются спонсорством для перехода на стадию песочницы, если не набирается необходимое большинство голосов для инкубационного этапа. Проект отклоняется, если недостаточно финансирования для квалификации на стадию песочницы. Этот метод выбора называется "fallback voting" (голосование с обратным отсчетом).

### Уровни зрелости проекта

#### Стадия песочницы
Эта стадия является начальной точкой для ранних проектов. Проекты в песочнице должны быть на ранней стадии развития и, по мнению TOC CNCF, заслуживать экспериментов. Песочница должна предоставлять полезный, нейтральный дом для таких проектов, чтобы способствовать совместной разработке. CNCF стремится сделать песочницу предпочтительным путем для ранних проектов. Более зрелые проекты могут переходить напрямую в инкубационный этап, но по мере роста облачно-нативной экосистемы ожидается увеличение доли ранних проектов.

#### Стадия инкубации
Проект должен удовлетворять требованиям стадии песочницы и пройти полное техническое due diligence, включая:

- Документация успешного использования проекта в производстве как минимум тремя независимыми прямыми пользователями
- Наличие здорового количества коммиттеров (тех, кто имеет право принимать вклад в проект)
- Демонстрация значительного и постоянного потока коммитов и объединенных вкладов
- Четкая схема версионирования
- Четко задокументированные процессы безопасности, включая порядок сообщения о проблемах безопасности и предоставления обновлений для устранения уязвимостей
- Для спецификаций необходима как минимум одна публичная эталонная реализация

#### Стадия выпуска
Для перехода из стадии песочницы или инкубации, или для нового проекта, который присоединяется в качестве выпускного проекта, проект должен удовлетворять критериям стадии инкубации плюс:

- Иметь коммиттеров как минимум из двух организаций
- Иметь и поддерживать знак качества Core Infrastructure Initiative Best Practices
- Пройти независимый сторонний аудит безопасности с опубликованными результатами, все критические уязвимости должны быть устранены до выпуска
- Явно определить процесс управления проектом и правила для коммиттеров
- Определить критерии, процесс и условия отставки или перехода в статус эмеритуса для поддерживающих проект; список поддерживающих должен храниться в файле MAINTAINERS.md и проверяться как минимум ежегодно
- Иметь публичный список пользователей проекта (например, ADOPTERS.md или логотипы на сайте проекта). Для спецификаций требуется список пользователей для их реализаций
- Получить квалифицированное большинство голосов от TOC для перехода на стадию выпуска

Проекты могут пытаться перейти напрямую из песочницы в выпуск, если они могут продемонстрировать достаточную зрелость. Проекты могут оставаться в инкубационном состоянии бесконечно долго, но обычно ожидается, что они перейдут на стадию выпуска в течение двух лет.

# РАБОТА С KUBERNETES

## Введение в главу

В этой главе мы узнаем о различных объектах Kubernetes, их назначении и о том, как с ними взаимодействовать.

После настройки кластера или подключения к существующему, мы можем начать развертывание рабочих нагрузок. Наименьшей вычислительной единицей в Kubernetes является не контейнер, а объект Pod. Тем не менее, Pod не является единственной абстракцией для управления рабочими нагрузками. Kubernetes предлагает множество объектов, которые контролируют то, как Pods развертываются, масштабируются и управляются.

Развертывание рабочих нагрузок — не единственная задача разработчика или администратора. Kubernetes предлагает решения для многих базовых проблем, связанных с контейнерами и оркестрацией: управление конфигурацией, сетевое взаимодействие между узлами, маршрутизация внешнего трафика, балансировка нагрузки и масштабирование Pod'ов.

## Объекты Kubernetes

Одной из фундаментальных концепций Kubernetes является предоставление множества абстрактных ресурсов, называемых объектами, которые используются для описания способа обработки рабочих нагрузок. Некоторые из них решают проблемы оркестрации контейнеров, такие как планирование и самовосстановление, другие предназначены для решения проблем, присущих контейнерам.

Объекты Kubernetes можно разделить на ориентированные на рабочие нагрузки, которые используются для управления контейнерными приложениями, и ориентированные на инфраструктуру, которые обрабатывают конфигурацию, сетевые взаимодействия и безопасность. Некоторые объекты могут быть размещены в пространствах имен (namespaces), в то время как другие доступны на уровне всего кластера.

Как пользователи, мы можем описывать эти объекты в формате YAML и отправлять их на API-сервер, где они проходят проверку перед созданием.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec: 
  selector:
    matchLabels:
      app: nginx
  replicas: 2 # сообщает деплойменту о запуске 2 pod'ов, соответствующих шаблону
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.19
        ports:
        - containerPort: 80
```

Обязательные поля включают:

- **apiVersion**: Каждый объект может быть версионирован, что означает возможность изменения структуры данных объекта между версиями.
- **kind**: Тип объекта, который должен быть создан.
- **metadata**: Данные для идентификации объекта. Имя обязательно для каждого объекта и должно быть уникальным в пределах пространства имен.
- **spec**: Спецификация объекта, где описывается желаемое состояние. Структура зависит от версии и типа объекта.

Создание, изменение или удаление объекта — это декларация намерений, в которой вы описываете желаемое состояние объектов. Вы не запускаете непосредственно pod'ы или контейнеры, как на локальной машине, и не получаете прямой обратной связи об успешности операции.

## Взаимодействие с Kubernetes

Для доступа к API Kubernetes используется официальный клиент командной строки — kubectl. Рассмотрим основные команды для повседневной работы с Kubernetes.

> **Примечание:** Инструкции по установке kubectl доступны в официальной документации.

Вы можете получить список доступных объектов в кластере с помощью команды:

```bash
$ kubectl api-resources

NAME                    SHORTNAMES  APIVERSION  NAMESPACED  KIND
...
configmaps              cm          v1          true        ConfigMap
...
namespaces              ns          v1          false       Namespace
nodes                   no          v1          false       Node
persistentvolumeclaims  pvc         v1          true        PersistentVolumeClaim
...
pods                    po          v1          true        Pod
...
services                svc         v1          true        Service
```

Обратите внимание, что объекты имеют сокращенные имена, что удобно для часто используемых команд. Таблица также показывает, какие объекты могут быть в пространстве имен и в какой версии API они доступны.

Для получения подробной информации об объекте kubectl предоставляет функцию explain:

```bash
$ kubectl explain pod

KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is     
     created by clients and scheduled onto hosts. 

FIELDS: 
   apiVersion <string>     
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal         
     value, and may reject unrecognized values.
...
   kind <string>
...
   metadata <Object>
...
   spec <Object>
```

Чтобы узнать больше о спецификации pod'а, можно использовать более глубокий запрос:

```bash
$ kubectl explain pod.spec

KIND:     Pod
VERSION:  v1 

RESOURCE: spec <Object>  

DESCRIPTION:
     Specification of the desired behavior of the pod. More info:

https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status      

     PodSpec is a description of a pod. 

FIELDS:
   activeDeadlineSeconds <integer>     
     Optional duration in seconds the pod may be active on the node relative to       
     StartTime before the system will actively try to mark it failed and kill         
     associated containers. Value must be a positive integer. 

   affinity <object>     
     If specified, the pod's scheduling constraints 

   automountServiceAccountToken <boolean>     
     AutomountServiceAccountToken indicates whether a service account token           
     should be automatically mounted. 

   containers <[]Object> -required-
...
```

Основные команды kubectl:

```bash
$ kubectl --help

kubectl controls the Kubernetes cluster manager. 

 Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/ 

Basic Commands (Beginner):
  create Create a resource from a file or from stdin
  expose Take a replication controller, service, deployment or pod and expose it as a new Kubernetes service
  run Run a particular image on the cluster
  set Set specific features on objects 

Basic Commands (Intermediate):
  explain Get documentation for a resource
  get Display one or many resources
  edit Edit a resource on the server
  delete Delete resources by file names, stdin, resources and names, or by resources and label selector
```

Для создания объекта в Kubernetes из YAML-файла используется команда:

```bash
kubectl create -f <your-file>.yaml
```

Или предпочтительнее:

```bash
kubectl apply -f <your-file>.yaml
```

Для Kubernetes существует множество графических интерфейсов и панелей управления, позволяющих визуально взаимодействовать с кластером:

- [kubernetes/dashboard](https://github.com/kubernetes/dashboard) — официальная веб-панель управления Kubernetes
- [derailed/k9s](https://github.com/derailed/k9s) — терминальный интерфейс для управления Kubernetes
- [Lens](https://k8slens.dev/) — десктопное приложение для управления кластерами Kubernetes
- [VMware Tanzu Octant](https://octant.dev/) — инструмент для визуализации кластеров Kubernetes

Помимо CLI и GUI существуют также продвинутые инструменты для создания шаблонов и пакетирования объектов Kubernetes. Вероятно, самый популярный — Helm, менеджер пакетов для Kubernetes, который упрощает обновления и управление объектами. Helm упаковывает объекты Kubernetes в Charts, которыми можно делиться через реестр. Для начала работы с Kubernetes вы можете искать готовые пакеты на [ArtifactHub](https://artifacthub.io/).

## Концепция Pod

Самый важный объект в Kubernetes — это Pod. Pod описывает единицу, состоящую из одного или нескольких контейнеров, которые разделяют слой изоляции (namespaces и cgroups). Это наименьшая развертываемая единица в Kubernetes, что означает, что Kubernetes не взаимодействует с контейнерами напрямую. Концепция Pod была введена для возможности запуска взаимозависимых процессов в рамках одной логической группы. Все контейнеры внутри Pod имеют один IP-адрес и могут совместно использовать файловую систему.

![Несколько контейнеров делят пространства имен для формирования Pod](https://kubernetes.io/docs/tutorials/_images/pod.svg)

### Пример простого объекта Pod с двумя контейнерами

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-with-sidecar
spec:
  containers:
  - name: nginx
    image: nginx:1.19
    ports:
    - containerPort: 80
  - name: count
    image: busybox:1.34
    args: [/bin/sh, -c,
            'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done']
```

Вы можете добавить сколько угодно контейнеров к вашему основному приложению, но при этом теряете возможность масштабировать их по отдельности. Использование второго контейнера, поддерживающего основное приложение, называется sidecar-контейнером.

Все определённые контейнеры запускаются одновременно без определенного порядка, но вы также можете использовать init-контейнеры для запуска подготовительных операций до запуска основного приложения. В следующем примере init-контейнер `init-myservice` проверяет доступность другого сервиса, и только после этого запускается основной контейнер.

### Пример Pod с init-контейнером

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
```

Важно ознакомиться с документацией о Pods, так как существует множество доступных настроек. Некоторые важные параметры, которые можно установить для каждого контейнера в Pod:

- **resources**: Установка запросов и лимитов ресурсов (CPU и память)
- **livenessProbe**: Настройка проверки работоспособности, которая периодически проверяет активность приложения и при необходимости перезапускает контейнер
- **securityContext**: Настройка параметров пользователя, группы и возможностей ядра для контейнера

## Жизненный цикл Pod

Pod'ы проходят определённый жизненный цикл, начиная с фазы Pending, переходя в фазу Running, если хотя бы один из основных контейнеров запускается успешно, и затем переходя либо в фазу Succeeded, либо в фазу Failed в зависимости от результата выполнения контейнеров.

### Фазы жизненного цикла Pod

#### Pending
Pod принят кластером Kubernetes, но один или несколько контейнеров не настроены и не готовы к запуску. Это включает время, которое Pod проводит в ожидании назначения на узел, а также время, затраченное на загрузку образов контейнеров по сети.

#### Running
Pod привязан к узлу, и все контейнеры созданы. По крайней мере один контейнер всё ещё работает или находится в процессе запуска или перезапуска.

#### Succeeded
Все контейнеры в Pod успешно завершили работу и не будут перезапускаться.

#### Failed
Все контейнеры в Pod завершили работу, и по крайней мере один контейнер завершился с ошибкой (ненулевой статус завершения или принудительное завершение системой).

#### Unknown
По какой-то причине состояние Pod не может быть получено, обычно из-за проблем связи с узлом, на котором должен выполняться Pod.

## Объекты нагрузки

Работа только с Pod'ами недостаточно гибка для платформы оркестрации контейнеров. Например, если Pod потерян из-за сбоя узла, он исчезает навсегда. Чтобы гарантировать, что определённое количество копий Pod всегда работает, используются объекты контроллеров, которые управляют Pod'ами.

### Объекты Kubernetes

### ReplicaSet

**Назначение:** Поддержание стабильного набора реплик подов, работающих в любой момент времени.

**Функции:**
- Обеспечивает указанное количество идентичных подов
- Автоматически заменяет поды при сбоях
- Масштабирует количество подов вверх или вниз по запросу

**Применение:** Обычно используется через Deployments, редко напрямую.

**Пример использования:** Обеспечение работы 3 реплик веб-сервера для отказоустойчивости.

### Deployment

**Назначение:** Декларативное обновление подов и ReplicaSets.

**Функции:**
- Управляет rollout (постепенное обновление) новых версий приложения
- Поддерживает rollback к предыдущим версиям
- Обеспечивает контроль над скоростью развертывания
- Автоматически создает и управляет ReplicaSets

**Применение:** Основной способ развертывания stateless приложений.

**Пример использования:** Обновление версии веб-приложения без простоя.

### StatefulSet

**Назначение:** Управление stateful приложениями.

**Функции:**
- Обеспечивает стабильные, уникальные сетевые идентификаторы
- Предоставляет стабильное постоянное хранилище
- Упорядоченное, плавное развертывание и масштабирование
- Упорядоченное, автоматическое обновление

**Применение:** Для приложений, требующих стабильных идентификаторов или постоянного хранилища, таких как базы данных.

**Пример использования:** Развертывание кластера баз данных, например, MySQL с репликацией.

### DaemonSet

**Назначение:** Обеспечение работы копии пода на каждом (или выбранных) узле кластера.

**Функции:**
- Автоматическое добавление подов на новые узлы
- Удаление подов при удалении узлов
- Обеспечение работы системных или служебных процессов на всех узлах

**Применение:** Для служб мониторинга, логирования, сетевых плагинов.

**Пример использования:** Установка агента сбора логов на каждом узле кластера.

### Job

**Назначение:** Выполнение разовой задачи до её завершения.

**Функции:**
- Создает один или несколько подов и обеспечивает их выполнение до успешного завершения
- Может быть настроен на параллельное выполнение
- Автоматически очищает завершенные поды

**Применение:** Для пакетных задач, обработки данных, миграций баз данных.

**Пример использования:** Выполнение скрипта обновления базы данных.

### CronJob

**Назначение:** Создание Jobs по расписанию.

**Функции:**
- Создает Job объекты в определенное время или интервал
- Поддерживает стандартный cron формат для определения расписания
- Управляет историей выполнения и очисткой завершенных Jobs

**Применение:** Для регулярных задач обслуживания, бэкапов, отправки отчетов.

**Пример использования:** Ежедневное резервное копирование базы данных в полночь.

## Сетевые объекты

Поскольку работа с большим количеством Pod'ов требует значительной ручной настройки сети, в Kubernetes используются объекты Service и Ingress для определения и абстрагирования сетевых взаимодействий.

### Типы сервисов

Сервисы в Kubernetes используются для экспонирования набора подов как сетевого сервиса. Они обеспечивают стабильный способ доступа к приложениям, работающим в подах, независимо от изменений в кластере.

Kubernetes предоставляет DNS для сервисов, позволяя компонентам обращаться друг к другу по именам сервисов без необходимости знать физические IP-адреса подов.

#### ClusterIP
ClusterIP — стандартный тип сервиса, создающий виртуальный IP-адрес, доступный только внутри кластера. Он обеспечивает:
- Стабильную точку доступа к группе подов
- Автоматическую балансировку нагрузки между подами
- Внутрикластерное DNS-имя

Kubernetes использует NAT для перенаправления входящего трафика на соответствующие поды.

**Пример:** Веб-приложение обращается к базе данных через ClusterIP сервис, используя его DNS-имя. Kubernetes автоматически распределяет запросы между подами базы данных.

#### NodePort
NodePort расширяет ClusterIP, открывая порт на всех узлах кластера (по умолчанию в диапазоне 30000-32767). Это позволяет получить доступ к сервису извне кластера.
NAT используется для перенаправления трафика с внешнего порта узла на внутренний ClusterIP сервис.

**Пример:** Игровой сервер, доступный через NodePort 31000, позволяет игрокам подключаться, используя IP любого узла кластера и этот порт.

#### LoadBalancer
Тип сервиса LoadBalancer расширяет NodePort, развертывая внешний балансировщик нагрузки. Это работает только в среде, которая имеет API для настройки балансировщика нагрузки, например, в облачных провайдерах GCP, AWS, Azure или даже OpenStack.

**Пример:** Для интернет-магазина в облаке создание LoadBalancer сервиса предоставляет внешний IP-адрес, который автоматически распределяет трафик между всеми подами, обеспечивая высокую доступность и масштабируемость.

#### ExternalName
Особый тип сервиса без маршрутизации. ExternalName использует внутренний DNS-сервер Kubernetes для создания DNS-алиаса. Вы можете использовать его для создания простого алиаса для разрешения сложного имени хоста, например: my-cool-database-az1-uid123.cloud-provider-i-like.com.

**Пример:** Для внешней базы данных с длинным и сложным DNS-именем можно создать ExternalName сервис с простым именем "mydb". Это позволит всем приложениям в кластере обращаться к базе данных, используя простое имя "mydb", вместо сложного внешнего адреса.

#### Headless Services
Headless сервисы не имеют ClusterIP. Они используются, когда не требуется балансировка нагрузки или единый IP-адрес сервиса. Для них:
- Не выделяется кластерный IP
- kube-proxy их не обрабатывает
- Нет балансировки нагрузки или проксирования

Часто применяются со StatefulSets для прямого доступа к отдельным подам.

**Пример:** Headless сервис для StatefulSet MongoDB позволяет приложениям обращаться к конкретным экземплярам базы данных напрямую, используя уникальные DNS-адреса подов.

### Ingress

Для более гибкого экспонирования приложений используется объект Ingress. Ingress обеспечивает HTTP и HTTPS маршрутизацию из внешней сети к сервисам в кластере. Он определяет правила маршрутизации, которые пользователь настраивает и реализует с помощью контроллера ingress.

Стандартные функции контроллеров ingress включают:

- Балансировку нагрузки
- TLS-терминацию
- Виртуальный хостинг на основе имени
- Маршрутизацию на основе пути URL

Многие контроллеры ingress предоставляют дополнительные функции:

- Перенаправления
- Настраиваемые ответы об ошибках
- Аутентификацию
- Привязку сессий
- Мониторинг
- Логирование
- Взвешенную маршрутизацию
- Ограничение скорости

### NetworkPolicy

Kubernetes также предоставляет внутрикластерный брандмауэр через NetworkPolicy. NetworkPolicy — это простой IP-брандмауэр (OSI уровни 3 или 4), который контролирует трафик на основе правил. Вы можете определять правила для входящего (ingress) и исходящего (egress) трафика. Типичное применение NetworkPolicy — ограничение трафика между разными пространствами имен.

## Объекты томов и хранения

Как упоминалось ранее, контейнеры не предназначены для постоянного хранения данных, особенно когда требуется хранение, распределенное между несколькими узлами. Kubernetes предлагает несколько решений, но они не устраняют полностью все сложности управления хранилищем в контейнерной среде.

Контейнеры уже имели концепцию монтирования томов, но поскольку мы не работаем с контейнерами напрямую, Kubernetes включил тома в спецификацию Pod, наряду с контейнерами.

### Пример монтирования тома hostPath

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # местоположение директории на хосте
      path: /data
      # это поле не является обязательным
      type: Directory
```

Тома позволяют делиться данными между несколькими контейнерами внутри одного Pod. Это обеспечивает гибкость при использовании паттерна sidecar. Другая цель томов — предотвратить потерю данных при аварийном завершении и перезапуске Pod на том же узле. Pod'ы запускаются в чистом состоянии, но все данные теряются, если они не записаны в том.

Кластерная среда с несколькими серверами требует еще большей гибкости для постоянного хранения. В зависимости от среды можно использовать облачное блочное хранилище (Amazon EBS, Google Persistent Disks, Azure Disk Storage) или распределенные системы хранения (Ceph, GlusterFS, NFS).

Это лишь некоторые из возможных типов хранилищ для Kubernetes. Для унификации пользовательского опыта Kubernetes использует интерфейс контейнерного хранилища (CSI), позволяющий поставщикам хранилищ создавать плагины (драйверы хранилища) для интеграции с Kubernetes.

### Абстракции для работы с хранилищем

#### PersistentVolume (PV)
Абстрактное описание единицы хранилища. Конфигурация объекта содержит информацию о типе тома, его размере, режиме доступа, уникальных идентификаторах и способе монтирования.

#### PersistentVolumeClaim (PVC)
Запрос на хранилище от пользователя. Если в кластере имеется несколько постоянных томов, пользователь может создать PVC для резервирования постоянного тома согласно своим требованиям.

### Пример

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-pv
spec:
  capacity:
    storage: 50Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  csi:
    driver: ebs.csi.aws.com
    volumeHandle: vol-05786ec9ec9526b67
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
    - name: app
      image: centos
      command: ["/bin/sh"]
      args:
        ["-c", "while true; do echo $(date -u) >> /data/out.txt; sleep 5; done"]
      volumeMounts:
        - name: persistent-storage
          mountPath: /data
  volumes:
    - name: persistent-storage
      persistentVolumeClaim:
        claimName: ebs-claim
```

Этот пример показывает PersistentVolume, использующий том AWS EBS через CSI-драйвер. После создания PersistentVolume разработчик может зарезервировать его с помощью PersistentVolumeClaim. Затем PVC используется в качестве тома в Pod.

## Объекты конфигурации

Методика «Двенадцати факторов» рекомендует хранить конфигурацию в среде. На практике запуск приложения требует больше, чем просто код и библиотеки. Приложения имеют конфигурационные файлы, строки подключения к другим сервисам, базам данных, хранилищам или кэшам.

Встраивание конфигурации непосредственно в образ контейнера — плохая практика. Любое изменение конфигурации потребует пересборки образа и повторного развертывания контейнера или pod. Эта проблема усугубляется при использовании нескольких сред (разработка, тестирование, производство), когда требуется создавать разные образы для каждой среды. Методика «Двенадцати факторов» подробно объясняет эту проблему: [Dev/prod parity](https://12factor.net/dev-prod-parity).

В Kubernetes эта проблема решается отделением конфигурации от Pod с помощью ConfigMap. ConfigMap может хранить целые конфигурационные файлы или переменные в виде пар ключ-значение. Существует два способа использования ConfigMap:

1. Монтирование ConfigMap в качестве тома в Pod
2. Отображение переменных из ConfigMap в переменные окружения Pod

### Пример ConfigMap с конфигурацией nginx

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-conf
data:
  nginx.conf: |
    user nginx;
    worker_processes 3;
    error_log /var/log/nginx/error.log;
    ...
    server {
        listen 80;
        server_name _;
        location / {
            root html;
            index index.html index.htm;
        }
    }
```

### Использование ConfigMap в Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.19
    ports:
    - containerPort: 80
    volumeMounts:
    - mountPath: /etc/nginx
      name: nginx-conf
  volumes:
  - name: nginx-conf
    configMap:
      name: nginx-conf
```

Kubernetes также предоставляет объект Secret для хранения конфиденциальной информации: паролей, ключей и других учетных данных. Secrets очень похожи на ConfigMap, но их значения кодируются в base64.

Следует отметить, что существует дискуссия о безопасности использования Secrets, так как они, вопреки названию, не считаются полностью защищенными. В облачных средах для управления секретами часто используются специализированные инструменты, такие как [HashiCorp Vault](https://www.vaultproject.io/).

## Объекты автоскалирования

Для масштабирования нагрузки в кластере Kubernetes используются три различных механизма автоскалирования.

### Механизмы автоскалирования в Kubernetes

#### Horizontal Pod Autoscaler (HPA)

**Описание:**  
Horizontal Pod Autoscaler автоматически масштабирует количество подов в deployment, replicaset или statefulset на основе наблюдаемой утилизации CPU или других выбранных метрик. Если поды превышают заданный порог утилизации, HPA создает дополнительные реплики для снижения нагрузки на каждый из подов.

**Механизм работы:**  
- HPA мониторит метрики (CPU, память) и сравнивает текущую утилизацию с пороговыми значениями, заданными в конфигурации
- При превышении порога HPA увеличивает количество подов, а когда утилизация падает ниже порога, количество подов может быть сокращено

#### Cluster Autoscaler

**Описание:**  
Cluster Autoscaler автоматически регулирует количество рабочих узлов в кластере. Он увеличивает число узлов при недостатке ресурсов для размещения подов и уменьшает, когда узлы недостаточно загружены.

**Механизм работы:**  
- Cluster Autoscaler мониторит нагрузку и загруженность узлов. Если поды не могут быть размещены из-за недостатка ресурсов, он добавляет новые узлы
- При недостаточной загрузке узлов и возможности перераспределить поды, система может уменьшить количество узлов для оптимизации затрат

#### Vertical Pod Autoscaler (VPA)

**Описание:**  
Vertical Pod Autoscaler автоматически настраивает запросы ресурсов и лимиты для подов на основе их исторического использования ресурсов. Это позволяет подам эффективнее использовать доступные ресурсы и предотвращает простои из-за их нехватки.

**Механизм работы:**  
- VPA анализирует потребление ресурсов подами и автоматически рекомендует или применяет изменения в их конфигурации запросов и лимитов
- Это может включать увеличение или уменьшение выделенной памяти и CPU в зависимости от изменения условий работы подов

### Как Cluster Autoscaler увеличивает количество узлов

**Основная функция:**  
Cluster Autoscaler автоматически регулирует количество узлов в кластере Kubernetes для адаптации к текущей нагрузке, добавляя узлы при необходимости и удаляя их, когда они избыточны.

**Механизм действия:**

1. **Мониторинг недостатка ресурсов:**
   - Cluster Autoscaler постоянно отслеживает поды, которые не могут быть запущены из-за недостатка ресурсов на существующих узлах
   - Проверяется невозможность размещения из-за недостатка CPU, памяти или других критических ресурсов

2. **Решение о масштабировании:**
   - При обнаружении недостаточности ресурсов Cluster Autoscaler инициирует добавление новых узлов
   - Алгоритм выбирает тип узла, оптимально соответствующий требованиям незапущенных подов

3. **Интеграция с облачными провайдерами:**
   - В облачных средах Cluster Autoscaler взаимодействует с API облачного провайдера для динамического добавления узлов
   - Используется конфигурация автоскейлинг-группы или аналогичные механизмы для управления жизненным циклом узлов

4. **Оптимизация затрат:**
   - После удовлетворения потребностей в ресурсах и стабилизации нагрузки, Cluster Autoscaler может определить недостаточно загруженные узлы и инициировать их удаление

Эффективность Cluster Autoscaler зависит от правильной настройки порогов масштабирования и понимания потребностей приложений в ресурсах.

### Требования для автоскалирования

Горизонтальное автоскалирование в Kubernetes не доступно "из коробки" и требует установки дополнения metrics-server.

Metrics-server можно заменить на Prometheus Adapter для Kubernetes Metrics APIs, что позволяет использовать пользовательские метрики и масштабировать приложения на основе таких параметров, как количество запросов или число пользователей.

### Автоскалирование на основе событий

Вместо использования только метрик, проекты типа KEDA (Kubernetes-based Event Driven Autoscaler) позволяют масштабировать нагрузку в Kubernetes на основе событий от внешних систем. KEDA, созданный в 2019 году совместно Microsoft и Red Hat, может масштабировать различные объекты: Deployments, ReplicaSets, Pods и Jobs. С обширным набором готовых скалеров KEDA реагирует на специальные триггеры, такие как запрос к базе данных или даже количество Pod в кластере Kubernetes.

## Объекты планирования

Планировщик (scheduler) — процесс, определяющий, на каких узлах будут запущены Pod'ы. Он выбирает подходящие узлы на основе ограничений и доступных ресурсов. Затем планировщик ранжирует каждый подходящий узел и связывает Pod с оптимальным узлом. В кластере может быть несколько планировщиков; kube-scheduler — реализация по умолчанию.

Стандартный планировщик обычно эффективно распределяет Pod по узлам, однако в некоторых случаях требуется ограничить Pod определёнными узлами или задать предпочтения. Существует несколько способов сделать это, наиболее рекомендуемый — использование Label Selectors для упрощения выбора.

### Методы выбора узлов для планирования Pod

#### Поле nodeSelector, соответствующее меткам узла

**NodeSelector** — простой, но эффективный механизм в Kubernetes для ограничения выбора узлов для размещения подов. Это базовый способ управления размещением подов в кластере.

Для использования `nodeSelector` необходимо добавить это поле в спецификацию пода с указанием меток (labels), которые должны присутствовать на узле. Kubernetes затем планирует поды только на узлах, соответствующих всем указанным меткам.

#### Affinity и Anti-Affinity в Kubernetes

**Affinity** и **anti-affinity** расширяют возможности ограничений планирования, предоставляя более гибкие инструменты для управления размещением подов.

- **Affinity** указывает, что поды должны быть запланированы вместе на одном узле, в одной зоне или домене неисправности. Это полезно для повышения производительности за счет совместного использования ресурсов или уменьшения задержек.

- **Anti-affinity** требует разнесения подов по различным узлам или зонам, что повышает отказоустойчивость, например, при распределении экземпляров баз данных по разным физическим серверам.

Правила могут быть:
- **Мягкими (Soft/Preferred)**: Планировщик попытается соблюсти эти правила, но при невозможности pod всё равно будет запланирован
- **Жесткими (Hard/Required)**: Pod не будет размещен, если правила не могут быть полностью соблюдены

#### Поле nodeName
nodeName — прямой способ выбора узла, более явный чем affinity или nodeSelector. Если поле nodeName не пусто, планировщик игнорирует Pod, и kubelet на указанном узле пытается разместить Pod. Это отменяет использование nodeSelector и правил affinity/anti-affinity.

#### Ограничения на распределение Pod по топологии
Topological Pod распределение позволяет контролировать размещение подов по топологическим доменам (регионы, зоны, узлы). Это полезно для улучшения производительности, доступности и эффективности использования ресурсов.

### Taints и Tolerations в Kubernetes

#### Общий обзор
Механизмы **taints** и **tolerations** обеспечивают гибкий способ ограничения размещения подов на определенных узлах. Они дополняют nodeSelector, affinity и anti-affinity, но работают по противоположному принципу.

#### Taints
**Taints** (загрязнения) применяются к узлам и позволяют им "отвергать" поды без соответствующих tolerations. Taint на узле указывает, что только поды с соответствующей tolerability могут быть запланированы на этом узле.

- **Синтаксис taint:**
  ```bash
  kubectl taint nodes <node-name> key=value:effect
  ```
  Где `effect` может быть:
  - `NoSchedule`: Поды без соответствующей toleration не будут запланированы на узел
  - `PreferNoSchedule`: Kubernetes попытается избежать планирования подов без соответствующей toleration на узел
  - `NoExecute`: Поды без соответствующей toleration будут выселены с узла, если они уже запущены

#### Tolerations
**Tolerations** применяются к подам и позволяют им быть запланированными на узлах с соответствующими taints. Toleration в спецификации пода "соответствует" taint на узле, если ключи и эффекты совпадают.

#### Зачем использовать Taints и Tolerations?
Taints и tolerations позволяют администраторам контролировать, какие поды могут запускаться на определенных узлах, что важно для соблюдения требований безопасности, производительности или лицензионных ограничений. Этот подход помогает эффективнее использовать ресурсы и управлять приложениями с различными требованиями.

## Безопасность в Kubernetes

В распределенной системе, такой как Kubernetes, безопасность — комплексная и многогранная задача. Требуется постоянное внимание к поддержанию защищенности облачной системы, включая оборудование, программное обеспечение и конфигурацию всей среды. Поскольку все больше приложений мигрирует в облако, безопасность должна быть интегрирована начиная с фазы проектирования.

После обеспечения базовой защиты платформы, kube-apiserver предоставляет набор инструментов и настроек для ограничения доступа и формализации авторизации в понятной форме.

Учитывая сетевую интенсивность Kubernetes, важно обеспечить безопасность сети с использованием стандартных средств защиты периметра и реализовать шифрование трафика между подами, применять NetworkPolicy и другие защитные меры внутри кластера.

Минимизация базовых образов, использование неизменяемых контейнеров, а также статический и динамический анализ — важные аспекты безопасности, которые часто реализуются разработчиками и интегрируются в CI/CD процессы перед использованием образов в производстве.

Инструменты AppArmor и SELinux также рекомендуется использовать для дополнительной защиты среды от потенциально вредоносных контейнеров.

Безопасность не ограничивается настройками и конфигурацией. Это непрерывный процесс обнаружения проблем с использованием инструментов обнаружения вторжений и поведенческого анализа. Необходим постоянный цикл оценки, предотвращения, обнаружения и реагирования в соответствии с документированными и регулярно обновляемыми политиками безопасности.

### Доступ к API

Для выполнения любого действия в кластере Kubernetes необходимо получить доступ к API через три основных этапа.

#### 1. Аутентификация (токен)

Тип аутентификации определяется параметрами запуска kube-apiserver. Некоторые варианты конфигурации:

- --basic-auth-file
- --oidc-issuer-url
- --token-auth-file
- --authorization-webhook-config-file

Подробнее об аутентификации можно узнать в [официальной документации](https://kubernetes.io/docs/reference/access-authn-authz/authentication/).

#### 2. Авторизация (RBAC)

RBAC (Role Based Access Control) — система контроля доступа на основе ролей. Все ресурсы в Kubernetes моделируются как объекты API, принадлежащие к API-группам (core, apps и т.д.). Эти ресурсы поддерживают операции создания, чтения, обновления и удаления (CRUD).

В YAML-файлах операции называются глаголами (verbs). RBAC основан на следующих элементах:

- Правила (Rules) — операции, действующие на API-группу
- Роли (Roles) — группы правил для одного пространства имен, ClusterRoles — для всего кластера
- Субъекты: учетные записи пользователей, сервисные аккаунты и группы

RBAC позволяет определить правила разрешения или запрета операций для пользователей, ролей или групп на различные ресурсы.

#### 3. Admission Controllers

Последний этап для разрешения запроса API — контроль доступа (Admission Control).

Admission controllers — компоненты, которые могут получать доступ к содержимому объектов, создаваемых запросами. Они могут изменять содержимое, проверять его и потенциально отклонять запрос.

Некоторые функции Kubernetes требуют определенных контроллеров доступа. Начиная с версии 1.12, kube-apiserver использует встроенный набор контроллеров, которые можно включать или отключать.

Один из контроллеров — Initializers, который позволяет динамически изменять запросы API. Другой пример — ResourceQuota, следящий за тем, чтобы создаваемый объект не нарушал существующие квоты ресурсов.

Для управления контролем доступа внешне можно использовать инструменты вроде Open Policy Agent (OPA) — движка политик общего назначения с открытым исходным кодом. OPA предоставляет декларативный язык для определения политик как код и API для принятия решений о политиках. Его можно применять для обеспечения политик в микросервисах, Kubernetes, CI/CD, API-шлюзах и других системах.

# ДОСТАВКА ОБЛАЧНЫХ ПРИЛОЖЕНИЙ

## Введение в главу

Развертывание приложений значительно эволюционировало за последние десятилетия: от физической передачи носителей до современных автоматизированных процессов, когда код собирается на сервере, упаковывается в контейнер и сразу развертывается на платформах вроде Kubernetes.

Способы доставки приложений тесно связаны с движением DevOps, которое появилось в конце 2000-х годов. DevOps представляет собой культурный сдвиг, который внедрил в индустрию множество новых методологий и технологий.

Возможно, самым важным изменением стала автоматизация процесса развертывания, позволяющая быстрее, чаще и с более высоким качеством доставлять программное обеспечение. В этой главе рассматриваются методы Непрерывной Интеграции/Непрерывной Доставки (CI/CD) и их развитие в новых инструментах и практиках, таких как GitOps.

## Основы доставки приложений

Жизненный цикл каждого приложения начинается с написания кода. Исходный код — не только основа приложения, но и интеллектуальная собственность, являющаяся капиталом большинства компаний и разработчиков. Управление исходным кодом эффективнее всего осуществляется через систему контроля версий.

В 2005 году Линус Торвальдс создал Git — систему контроля версий, ставшую сегодня индустриальным стандартом. Git — децентрализованная система для отслеживания изменений в коде. Она позволяет работать с копиями кода в ветках или форках до момента их объединения в основную ветку.

Рекомендуется ознакомиться с [официальным сайтом Git](https://git-scm.com/) для глубокого понимания этого мощного инструмента, который ежедневно используется практически всеми разработчиками и администраторами.

После помещения кода под контроль версий следующим шагом перед доставкой является его сборка, которая может включать создание образа контейнера, как описано в главе "Оркестрация контейнеров".

Для обеспечения высокого качества приложения необходимо проводить комплексное автоматическое тестирование, чтобы убедиться в сохранении функциональности после внесения изменений.

Завершающий шаг — доставка приложения на целевую платформу. Для Kubernetes это может включать написание YAML-манифеста и загрузку образа контейнера в реестр, откуда Kubernetes сможет его загрузить.

В современных практиках не только исходный код управляется системой контроля версий. Для полного использования облачных ресурсов широко применяется концепция инфраструктуры как кода (IaC). Вместо ручной настройки инфраструктура описывается в файлах и создается через API облачного провайдера, что позволяет разработчикам активнее участвовать в её конфигурировании.

## CI/CD

С уменьшением размера сервисов и увеличением частоты развертываний автоматизация процесса развертывания стала необходимостью. DevOps подчеркивает важность частых и быстрых обновлений. В традиционных условиях развертывание требовало участия разработчиков и администраторов, включало множество подверженных ошибкам ручных операций и сопровождалось постоянными опасениями сбоев.

Автоматизация — ключ к преодолению этих проблем. Сегодня широко применяются принципы Непрерывной Интеграции/Непрерывной Доставки (CI/CD), описывающие различные этапы развертывания приложения, конфигурации и даже инфраструктуры.

### Непрерывная Интеграция

Непрерывная интеграция (Continuous Integration) — первая часть процесса, описывающая постоянную сборку и тестирование написанного кода. Высокий уровень автоматизации и использование системы контроля версий позволяют нескольким разработчикам и командам эффективно работать с общей кодовой базой.

### Непрерывная Доставка

Непрерывная доставка (Continuous Delivery) — вторая часть процесса, автоматизирующая развертывание предварительно собранного программного обеспечения. В облачных средах программное обеспечение обычно сначала развертывается в средах разработки или тестирования перед выпуском в производственную систему.

### CI/CD Pipeline

Для автоматизации всего рабочего процесса используется конвейер CI/CD — последовательность скриптов, выполняемых на сервере или в контейнере. Конвейеры интегрируются с системой контроля версий, управляющей изменениями кодовой базы.

При готовности новой версии кода к развертыванию конвейер запускает скрипты, выполняющие сборку, тестирование, развертывание и проверки безопасности и соответствия.

Современные инструменты CI/CD обладают многими дополнительными функциями, включая прямую интеграцию с Kubernetes и другими платформами.

### Популярные инструменты CI/CD

- **Spinnaker** — платформа непрерывной доставки с открытым исходным кодом для развертывания приложений в различных облачных средах
- **GitLab** — полная DevOps платформа с интегрированными инструментами CI/CD
- **Jenkins** — ведущий сервер автоматизации с открытым исходным кодом
- **Jenkins X** — CI/CD решение для современных облачных приложений на Kubernetes
- **Tekton CD** — Kubernetes-нативный фреймворк для создания CI/CD систем
- **ArgoCD** — декларативный GitOps инструмент непрерывной доставки для Kubernetes

Для дополнительной информации о DevOps, SRE и инфраструктуре как коде рекомендуется курс [Introduction to DevOps and Site Reliability Engineering (LFS162x)](https://www.edx.org/course/introduction-to-devops-and-site-reliability-engineering) на edX.

## GitOps

Инфраструктура как код (IaC) революционизировала подход к созданию и управлению инфраструктурой, повысив качество и скорость её предоставления. Сегодня конфигурации, сетевые настройки, политики и системы безопасности могут быть описаны как код и храниться в том же репозитории, что и программное обеспечение.

GitOps развивает эту концепцию, принимая Git как единственный источник истины и интегрируя процессы предоставления и модификации инфраструктуры с операциями контроля версий.

Если код разделен на ветки и должен быть объединен обратно в основную ветку, создается запрос на слияние (pull request), который рассматривается другими разработчиками перед фактическим слиянием. Эта практика давно используется в разработке ПО и включает запуск конвейера CI для каждого изменения. В GitOps эти запросы на слияние управляют изменениями инфраструктуры.

Основная цель GitOps — автоматизация обеспечения инфраструктуры и развертывания приложений. "Неизменяемая инфраструктура" в этом контексте означает инфраструктуру, которая заменяется, а не обновляется на месте.

### Push-based

В Push-based подходе конвейер запускается и использует инструменты для внесения изменений в платформу. Изменения могут быть инициированы коммитом или запросом на слияние.

### Pull-based

В Pull-based подходе агент отслеживает изменения в Git-репозитории и сравнивает определение в репозитории с актуальным состоянием. При обнаружении различий агент автоматически применяет изменения к инфраструктуре.

Популярные Pull-based фреймворки GitOps — Flux и ArgoCD. ArgoCD реализован как контроллер Kubernetes, а Flux построен на GitOps Toolkit, наборе API и контроллеров для расширения возможностей или создания собственной платформы доставки.

Архитектура ArgoCD демонстрирует типичную реализацию GitOps:

![Архитектура ArgoCD](https://argo-cd.readthedocs.io/en/stable/assets/argocd_architecture.png)

Kubernetes особенно хорошо подходит для GitOps благодаря своему API и декларативному подходу к предоставлению ресурсов и изменениям. В основе Kubernetes лежит принцип, сходный с Pull-based подходом: отслеживаются изменения состояния, и система автоматически адаптируется для достижения желаемого состояния.

Для практического знакомства с GitOps, ArgoCD и Flux рекомендуется курс [Introduction To GitOps (LFS169)](https://www.edx.org/course/introduction-to-gitops) на edX.

# НАБЛЮДАЕМОСТЬ В ОБЛАЧНЫХ СРЕДАХ

## Введение в главу

Термин "наблюдаемость в облачных средах" иногда воспринимается как модное слово для продвижения новых инструментов. В этом есть доля правды, поскольку появилось множество решений для мониторинга контейнерной инфраструктуры.

Традиционный мониторинг серверов включает сбор метрик системы (CPU, память) и ведение журналов процессов и операционной системы. Новый вызов для микросервисной архитектуры — отслеживание запросов, проходящих через распределённую систему. Эта дисциплина называется трассировкой и особенно полезна, когда в обработке запроса участвуют многочисленные сервисы.

В этой главе рассматривается, как инфраструктура контейнеров по-прежнему опирается на сбор метрик и логов, но с существенно изменившимися требованиями. Больше внимания уделяется сетевым параметрам (задержка, пропускная способность, повторные запросы, время запуска), а огромные объемы данных в распределённых системах требуют иного подхода к управлению.

## Наблюдаемость

Наблюдаемость не является синонимом мониторинга; мониторинг — лишь одна из составляющих наблюдаемости в облачных средах. Термин "наблюдаемость" тесно связан с теорией управления, описывающей поведение динамических систем. По сути, теория управления показывает, как внешние выходы систем могут быть измерены для контроля их поведения.

Показательный пример — система круиз-контроля автомобиля. Водитель задает желаемую скорость, которая постоянно измеряется и отображается на спидометре. Для поддержания скорости в изменяющихся условиях, например при подъеме в гору, мощность двигателя автоматически корректируется.

В ИТ-системах аналогичный принцип применяется при автоскалировании: устанавливается желаемая нагрузка, и события масштабирования запускаются на основе её измерений.

Автоматизация таким образом может быть сложной задачей, но основное применение наблюдаемости — отслеживание систем, их взаимодействия и поведения под нагрузкой или при сбоях

Наблюдаемость должна давать ответы на такие вопросы, как:

- Стабильна ли система или она меняет состояние при внешних воздействиях?
- Насколько чувствительна система к изменениям, например, когда некоторые сервисы имеют высокую задержку?
- Превышают ли определенные метрики свои пороговые значения?
- Почему запрос к системе завершился неудачей?
- Есть ли в системе узкие места?

Высшая цель наблюдаемости — обеспечить возможность анализа собранных данных для лучшего понимания системы и реагирования на ошибки. Технологический аспект наблюдаемости тесно связан с современной гибкой разработкой, которая также использует циклы обратной связи для анализа поведения программного обеспечения и его постоянной адаптации на основе полученных результатов.

## Телеметрия

Термин "телеметрия" происходит от греческих слов "теле" (удаленный) и "метрия" (измерение). Он описывает процесс измерения и сбора данных с последующей их передачей в другую систему для обработки. Эта концепция не ограничивается облачными или ИТ-системами. Метеостанция с регистратором данных — наглядный пример, где измеряются температура, влажность, скорость ветра и другие параметры, а затем передаются для обработки и визуализации.

В контейнерных системах каждое приложение должно иметь встроенные инструменты для генерации информации, которая затем собирается и передается в централизованную систему. Данные телеметрии можно разделить на три категории:

### Логи

Это текстовые сообщения, генерируемые приложением при возникновении ошибок, предупреждений или для отладочной информации. Простой лог может фиксировать начало и завершение определенной операции, выполняемой приложением. Логи обычно структурированы в виде последовательных записей о событиях с метками времени.

### Метрики

Метрики представляют количественные измерения, собираемые с течением времени. Это могут быть показатели производительности, такие как количество запросов в секунду, время отклика, процент ошибок, использование ресурсов (CPU, память, диск) или бизнес-показатели, например, количество транзакций. Метрики обычно хранятся как временные ряды и используются для анализа тенденций и аномалий.

### Трассировки

Трассировки отслеживают прохождение запроса через распределенную систему. Они предоставляют детальную информацию о времени обработки запроса каждым сервисом и позволяют идентифицировать узкие места в цепочке взаимодействий микросервисов. Трассировка особенно важна в сложных распределенных системах, где запрос может проходить через десятки сервисов.

В традиционных системах данные, такие как логи, часто не передавались в централизованную систему, и для их просмотра приходилось подключаться к каждому серверу и читать файлы напрямую. В распределенной системе с сотнями или тысячами сервисов такой подход неэффективен и значительно затрудняет диагностику проблем.

## Логирование

Современные фреймворки и языки программирования имеют встроенные инструменты для ведения журналов, которые позволяют легко записывать информацию в файл с различными уровнями детализации.

Пример из документации Python:

```python
import logging
logging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)
logging.debug('Это сообщение должно попасть в лог файл')
logging.info('Это тоже должно попасть в лог')
logging.warning('И это тоже')
logging.error('И даже не-ASCII символы, такие как Øresund и Malmö')
```

В Unix и Linux программы используют три стандартных потока для ввода-вывода, из которых два применяются для вывода логов из контейнера:

- стандартный ввод (stdin): ввод в программу, например, с клавиатуры
- стандартный вывод (stdout): вывод программы на экран
- стандартный поток ошибок (stderr): ошибки, которые программа выводит на экран

Инструменты командной строки (docker, kubectl, podman) предоставляют команды для просмотра логов контейнеризованных процессов, если приложение выводит их в консоль или в /dev/stdout и /dev/stderr.

Пример просмотра логов контейнера nginx:

```bash
$ docker logs nginx
/docker-entrypoint.sh: /docker-entrypoint.d/ не пуст, попытаемся выполнить конфигурацию
/docker-entrypoint.sh: Ищем shell-скрипты в /docker-entrypoint.d/
/docker-entrypoint.sh: Запуск
/docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Получение контрольной суммы /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Включён listen на IPv6 в /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Запуск
/docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Запуск
/docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Конфигурация завершена; готов к запуску
2021/10/20 13:22:44 [notice] 1#1: использование метода событий "epoll"
2021/10/20 13:22:44 [notice] 1#1: nginx/1.21.3
```

Для наблюдения за логами в реальном времени можно добавить параметр `-f`. Kubernetes предоставляет аналогичную функциональность через команду `kubectl logs`. Примеры из документации:

```bash
# Вывод снимка логов из пода nginx с одним контейнером
kubectl logs nginx 

# Вывод логов завершённого контейнера ruby из пода web-1
kubectl logs -p -c ruby web-1 

# Потоковая передача логов контейнера ruby в поде web-1
kubectl logs -f -c ruby web-1 

# Отображение только последних 20 строк вывода в поде nginx
kubectl logs --tail=20 nginx 

# Просмотр всех логов из пода nginx за последний час
kubectl logs --since=1h nginx
```

Эти методы позволяют напрямую взаимодействовать с отдельным контейнером. Однако для управления большими объемами данных логи необходимо отправлять в централизованную систему хранения и анализа.

### Логирование на уровне узла

Наиболее эффективный способ сбора логов — использование агентов на каждом узле кластера. Такие агенты как Fluentd, Fluent Bit, Logstash или Filebeat устанавливаются на узлы и собирают логи со всех подов, запущенных на них. Затем данные отправляются в централизованное хранилище, такое как Elasticsearch, откуда они могут быть проанализированы с помощью инструментов вроде Kibana.

Стандартный стек для работы с логами в Kubernetes включает:
- **EFK (Elasticsearch, Fluentd, Kibana)** или 
- **ELK (Elasticsearch, Logstash, Kibana)**

Эти компоненты работают вместе, обеспечивая полный цикл обработки логов: сбор, транспортировку, хранение, поиск и визуализацию.

## Prometheus

Prometheus — это система мониторинга с открытым исходным кодом, разработанная изначально в SoundCloud и ставшая вторым проектом CNCF в 2016 году. Она превратилась в стандартный инструмент для мониторинга, который особенно хорошо интегрируется с экосистемой Kubernetes и контейнеров.

Prometheus собирает метрики, генерируемые приложениями и серверами в виде временных рядов — наборов данных, включающих временную метку, метку и само измерение. Модель данных Prometheus предоставляет четыре основных типа метрик:

- **Counter**: значение, которое может только увеличиваться (например, количество запросов или ошибок)
- **Gauge**: значение, которое может увеличиваться или уменьшаться (например, использование памяти)
- **Histogram**: выборка наблюдений с распределением по диапазонам (например, продолжительность запросов)
- **Summary**: подобно гистограмме, но также предоставляет общее количество наблюдений и сумму всех значений

Для предоставления метрик приложения могут экспонировать HTTP-эндпоинт `/metrics`. Вместо самостоятельной реализации можно использовать готовые клиентские библиотеки для различных языков:

- Go
- Java или Scala
- Python
- Ruby
- И множество неофициальных библиотек для других языков

Пример выходных данных может выглядеть так:

```plaintext
# HELP queue_length The number of items in the queue.
# TYPE queue_length gauge
queue_length 42
# HELP http_requests_total The total number of handled HTTP requests.
# TYPE http_requests_total counter
http_requests_total 7734
# HELP http_request_duration_seconds A histogram of the HTTP request durations in seconds.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{le="0.05"} 4599
http_request_duration_seconds_sum 88364.234
http_request_duration_seconds_count 227420
# HELP http_request_duration_seconds A summary of the HTTP request durations in seconds.
# TYPE http_request_duration_seconds summary
http_request_duration_seconds{quantile="0.5"} 0.052
http_request_duration_seconds_sum 88364.234
http_request_duration_seconds_count 227420
```

Prometheus имеет встроенную интеграцию с Kubernetes и может автоматически обнаруживать все сервисы в кластере, собирая метрики с заданным интервалом и сохраняя их в базе данных временных рядов.

Для запросов к данным используется PromQL (Prometheus Query Language). Пользователи могут выбирать и агрегировать данные в реальном времени и просматривать результаты через встроенный интерфейс в графическом или табличном виде.

Примеры запросов PromQL:

```promql
# Все временные ряды метрики http_requests_total с указанными метками
http_requests_total{job="apiserver", handler="/api/comments"}

# Скорость изменения метрики http_requests_total за последние 5 минут
rate(http_requests_total[5m])
```

Часто используемым компаньоном Prometheus является Grafana — платформа для создания информативных дашбордов на основе собранных метрик. Хотя Grafana поддерживает множество источников данных, Prometheus — один из самых популярных.

Другой важный инструмент экосистемы — Alertmanager. Prometheus позволяет настраивать оповещения при достижении метриками определенных пороговых значений. Когда срабатывает оповещение, Alertmanager может отправить уведомления по различным каналам: в мессенджеры, по электронной почте или в специализированные системы управления инцидентами.

Пример правила оповещения:

```yaml
groups:
- name: example
  rules:
  - alert: HighRequestLatency
    expr: job:request_latency_seconds:mean5m{job="myjob"} > 0.5
    for: 10m
    labels:
      severity: page
    annotations:
      summary: High request latency
```

## Трассировка

Логирование и сбор метрик — традиционные подходы в мониторинге. Однако распределенная трассировка — относительно новая концепция, ставшая критически важной для микросервисных архитектур. Метрики и логи дают представление об отдельных сервисах, но для понимания прохождения запроса через всю систему необходима трассировка.

Трассировка отслеживает запрос по всему пути через распределенную систему и состоит из множества единиц работы ("spans"), представляющих различные события в его обработке. Каждое приложение может добавить свой сегмент к трассировке, включая время начала и окончания, название операции, теги или сообщения логов.

Трассировки сохраняются и анализируются в специализированных системах, таких как Jaeger или Zipkin. Они позволяют визуализировать поток запросов между сервисами, выявлять узкие места и аномалии в производительности.

В 2019 году проекты OpenTracing и OpenCensus объединились, создав OpenTelemetry — проект CNCF для стандартизации телеметрии. OpenTelemetry предоставляет API, SDK и инструменты для интеграции всех видов телеметрии (метрик, логов и трассировок) в приложения и инфраструктуру. Клиентские библиотеки OpenTelemetry экспортируют данные в стандартизированном формате на централизованные платформы, такие как Jaeger, Zipkin или другие системы мониторинга.

## Управление затратами

Облачные вычисления предоставляют доступ к практически безграничным ресурсам с оплатой по мере использования. Эффективная оптимизация затрат основана на анализе реальных потребностей и автоматизации планирования необходимых ресурсов.

### Автоматическая и ручная оптимизация

#### Определение неиспользуемых и незадействованных ресурсов
С помощью хорошего мониторинга легко выявить неиспользуемые ресурсы или серверы с низкой загрузкой. Многие облачные провайдеры предоставляют инструменты анализа затрат, которые детализируют расходы по отдельным сервисам. Автоматическое масштабирование позволяет отключать ненужные экземпляры в периоды низкой нагрузки.

#### Right-Sizing - Оптимизация размера
На начальных этапах часто выбираются серверы с избыточной мощностью. Мониторинг со временем дает представление о реальных потребностях приложения. Оптимизация размера — непрерывный процесс адаптации к фактической нагрузке. Нет смысла платить за мощные машины, если используется только половина их возможностей.

#### Reserved Instances - Резервирование ресурсов
Модели ценообразования по запросу идеальны для непредсказуемых нагрузок, но дороги при постоянном использовании. Резервирование ресурсов и предоплата значительно снижают затраты. Этот подход эффективен при хорошем понимании необходимых ресурсов на длительный период, возможно, на годы вперед.

#### Spot Instances - Спотовые экземпляры
Для пакетных заданий или кратковременной повышенной нагрузки выгодно использовать спотовые экземпляры — неиспользуемые ресурсы, предлагаемые провайдерами по сниженным ценам. Особенность таких ресурсов в том, что они не гарантированы и могут быть прерваны с коротким уведомлением, когда понадобятся клиентам, платящим полную стоимость.

Эти стратегии можно комбинировать для максимальной экономической эффективности. Обычно нет противоречий в использовании смешанного подхода, включающего экземпляры по запросу, зарезервированные и спотовые.

# KUBERNETES В ДВУХ СЛОВАХ

## Ключевые концепции и инструменты

- **API группы**: Организуют функциональность API Kubernetes по категориям и версиям.
- **Команда отката развертывания**: `kubectl rollout undo` позволяет вернуться к предыдущей версии.
- **Канареечное развертывание**: Стратегия выпуска, направляющая часть трафика на новую версию для тестирования.
- **Headless Service**: Сервис без ClusterIP, предоставляющий DNS-записи для отдельных подов, необходим для StatefulSet.
- **NodePort**: Тип сервиса, открывающий порт на всех узлах кластера для внешнего доступа.
- **Network Policies**: Действуют как брандмауэр на уровнях 3 и 4 модели OSI.
- **CRI (Container Runtime Interface)**: Позволяет выбирать различные среды выполнения контейнеров.
- **reclaimPolicy в StorageClass**: Поддерживает стратегии `Delete` и `Retain` для управления жизненным циклом томов.
- **Компоненты service mesh**: Data Plane (передача данных) и Control Plane (управление).
- **Кластерная сеть**: Все контейнеры в поде используют общий IP-адрес.
- **Custom Resource Definitions (CRDs)**: Расширяют API Kubernetes для работы с пользовательскими ресурсами.

## Популярные инструменты экосистемы

- **Envoy**: Высокопроизводительный L7 прокси для микросервисных архитектур.
- **Open Policy Agent (OPA)**: Универсальный механизм политик для управления доступом и соответствием.
- **Knative**: Платформа для разработки и развертывания бессерверных приложений на Kubernetes.
- **Rook**: Оператор хранилища, интегрирующий различные системы хранения с Kubernetes.
- **Istio**: Полнофункциональная сервисная сетка для управления трафиком, безопасностью и наблюдаемостью.
- **Prometheus**: Система мониторинга и сбора метрик с поддержкой мощного языка запросов.
- **Helm**: Менеджер пакетов для Kubernetes, упрощающий установку и управление приложениями.
- **ArgoCD**: Инструмент для декларативной непрерывной доставки, реализующий принципы GitOps.

## Рекомендации по масштабированию

- **Минимальный отказоустойчивый кластер**: 3 узла контрольной плоскости и минимум 2 рабочих узла.
- **Максимальные ограничения стандартной конфигурации**:
  - До 5,000 узлов в кластере
  - До 150,000 подов в кластере
  - До 300,000 контейнеров
  - Максимум 100 подов на узел (по умолчанию)
- **Автомасштабирование**:
  - Horizontal Pod Autoscaler (HPA) для масштабирования количества подов
  - Vertical Pod Autoscaler (VPA) для оптимизации запросов ресурсов
  - Cluster Autoscaler для динамического управления количеством узлов
- **Хранение**:
  - Использование динамического предоставления томов через StorageClass
  - Предпочтение облачных нативных решений для хранения данных
  - Резервное копирование и восстановление с помощью специализированных инструментов (Velero)
- **Сеть**:
  - Выбор производительного CNI-плагина (Calico, Cilium)
  - Применение Network Policies для контроля трафика
  - Использование сервисных сеток для сложных сценариев маршрутизации

Kubernetes продолжает быстро развиваться, предлагая всё более мощные инструменты для создания, масштабирования и управления контейнеризованными приложениями. Понимание фундаментальных концепций и экосистемы Kubernetes — ключ к эффективной работе с современными облачными нативными архитектурами.
